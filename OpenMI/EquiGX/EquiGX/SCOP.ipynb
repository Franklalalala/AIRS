{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f44c0a4-aa7b-43b0-b5a7-5cc396499ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_path = \"/data/hongyiling/XAI4e3nn/dataset/scope/training/d1aoga3.hdf5\"\n",
    "h5File = h5py.File(hdf5_file_path, \"r\")\n",
    "structure = create_structure_from_hdf5(hdf5_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab2235cf-0e2c-48e4-835e-79f169afe6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['amino_chains', 'amino_neighs', 'amino_neighs_hb', 'amino_neighs_sindices', 'amino_neighs_sindices_hb', 'amino_pos', 'amino_types', 'atom_amino_id', 'atom_chain_ids', 'atom_chain_names', 'atom_names', 'atom_pos', 'atom_residue_id', 'atom_residue_names', 'atom_types', 'cov_bond_list', 'cov_bond_list_hb', 'cov_bond_list_sindices', 'cov_bond_list_sindices_hb', 'pos_center']>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5File.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69573c39-6c92-4a8d-b665-cd674a0e584c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10, 16,  2, 13,  4, 15,  9,  8,  1,  6,  7,  4, 16,  4,  2, 10,\n",
       "       19, 10, 19, 16, 10,  9,  1, 11,  8, 10, 18, 16, 10,  8,  8, 19, 13,\n",
       "        8,  8,  7, 11, 16,  1, 10, 10,  4,  8,  4, 15,  4, 14, 18, 15, 14,\n",
       "        2, 14,  6, 14,  7,  2, 16, 10, 19, 14,  8,  0,  4,  6,  8, 19,  4,\n",
       "        6, 16,  4, 19,  6, 14, 18,  4,  8,  1,  8,  9, 16, 16,  1,  8, 18,\n",
       "       10,  2,  8, 19,  3,  9,  9,  0,  6, 10,  4,  8,  1, 19,  2, 10,  3,\n",
       "       16,  8,  8,  8, 15, 10, 18, 12,  8, 17, 14, 10,  1, 15, 16, 13, 19,\n",
       "        1,  8,  2,  3,  2,  6, 11,  6, 16,  6, 13,  9, 10, 10,  1,  4, 10,\n",
       "       18,  8, 19,  4,  9,  4, 13,  6,  9,  7,  4, 10,  6, 10, 12, 19,  1,\n",
       "        3,  1, 12, 18,  6,  4,  2,  9, 10, 14,  4, 13,  9,  6,  3,  9,  1,\n",
       "       19, 15, 15, 11, 15, 16, 13,  6,  0, 10, 14, 12,  0, 11,  7,  9,  8,\n",
       "       19,  8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5File['amino_types'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e6a544a-76e1-476b-b836-523fd3b1db8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( h5File['amino_types'][:]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34262040-ade2-4837-837b-f4853cabb366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(h5File[\"atom_residue_id\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05c32410-1abc-458f-a64f-947eef884d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "residue1 = next(iter(structure.get_residues()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a24c8613-52a3-4b23-bdb1-67f3ca405cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('temp_structure', 0, '0', (' ', 0, ' '))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residue1.get_full_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bfabcfdb-aeec-4f75-a0ca-b35dac1612be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' ', 0, ' ')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residue1.get_id()[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fbbc575-e6ed-4f40-87f2-2a87583be073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'SER', b'SER', b'SER', b'SER', b'SER', b'SER', b'LEU', b'LEU',\n",
       "       b'LEU', b'LEU', b'LEU'], dtype='|S3')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5File['atom_residue_names'][:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e7a6471-6b12-4e0e-9bf2-de346b14f556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([1 for _ in structure.get_residues()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d62cd1b6-c75c-48d9-aa5a-d2d0533f7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dssp_executable_path = 'mkdssp'\n",
    "output_pdb_file_name = 'output_structure.pdb'\n",
    "\n",
    "write_structure_to_pdb(structure, output_pdb_file_name)\n",
    "\n",
    "parser = PDBParser()\n",
    "structure = parser.get_structure('temp_structure', output_pdb_file_name)\n",
    "model = structure[0]  # Assuming you want to analyze the first model\n",
    "\n",
    "dssp_result = dssp_dict_from_pdb_file(output_pdb_file_name)\n",
    "\n",
    "# Run DSSP analysis\n",
    "# label = assign_secondary_structure_with_dssp(output_pdb_file_name, dssp_executable_path)\n",
    "# label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f3535079-7cfd-40db-a1e6-a61ee3a8b8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dssp_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0e0511e-2d7b-445f-9c5d-025a9640488c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('S', '-', 117, 360.0, -29.2, 1, 0, 0.0, 2, -0.3, 0, 0.0, 62, -0.2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dssp_result[0][('0', (' ', 0, ' '))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfc688cd-d1f5-4499-ba16-c30eab986011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05355ec-080c-49db-bb18-a093795ddada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff22a4f-d577-4754-82f1-971da3dfa551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                         | 115/12312 [00:14<16:13, 12.53it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      "  1%|▊                                                                                         | 117/12312 [00:14<15:07, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1a1qa_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                                       | 327/12312 [00:39<29:15,  6.83it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      "  3%|██▍                                                                                       | 329/12312 [00:39<24:03,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d2zjr31.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▉                                                                                       | 397/12312 [00:48<30:34,  6.50it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      "  3%|██▉                                                                                       | 401/12312 [00:49<20:44,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1hrba_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      "  3%|██▉                                                                                       | 404/12312 [00:49<17:08, 11.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d2mysb_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████▏                                                                                | 1125/12312 [02:19<20:02,  9.30it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      "  9%|████████▏                                                                                | 1127/12312 [02:19<23:13,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d2ilaa_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████                                                                              | 1524/12312 [03:10<22:07,  8.13it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 12%|███████████                                                                              | 1526/12312 [03:10<20:06,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1kvpa_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████▏                                                                            | 1689/12312 [03:32<20:38,  8.57it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 14%|████████████▏                                                                            | 1691/12312 [03:32<17:21, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d2mysc_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████▍                                                                            | 1715/12312 [03:36<22:29,  7.85it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 14%|████████████▍                                                                            | 1717/12312 [03:36<19:45,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1hioa_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▉                                                                            | 1798/12312 [03:47<23:39,  7.41it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 15%|█████████████                                                                            | 1800/12312 [03:47<19:03,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d2at2a1.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████▏                                                                    | 2801/12312 [05:56<16:54,  9.38it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d2rcja1.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████▉                                                                   | 3027/12312 [06:26<18:41,  8.28it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1xbpg2.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████████▍                                                            | 3937/12312 [08:23<21:31,  6.48it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 32%|████████████████████████████▍                                                            | 3941/12312 [08:24<14:46,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d2madh_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████████▉                                                        | 4548/12312 [09:43<21:43,  5.95it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 37%|████████████████████████████████▉                                                        | 4551/12312 [09:44<15:31,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1ffkf_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████████                                                      | 4847/12312 [10:22<15:33,  7.99it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1hmja_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████████                                                      | 4851/12312 [10:23<20:37,  6.03it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 39%|███████████████████████████████████                                                      | 4853/12312 [10:23<16:44,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1zvoa1.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████████████████████████████████████▏                                                    | 5002/12312 [10:43<14:28,  8.42it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1xbpg1.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████████▎                                                 | 5431/12312 [11:39<11:42,  9.79it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 44%|███████████████████████████████████████▎                                                 | 5433/12312 [11:39<10:11, 11.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1pcla_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████▋                                            | 6179/12312 [13:16<15:29,  6.60it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 50%|████████████████████████████████████████████▋                                            | 6181/12312 [13:16<12:59,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1hiob_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████████▋                                         | 6603/12312 [14:10<09:09, 10.39it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 54%|███████████████████████████████████████████████▋                                         | 6605/12312 [14:10<08:35, 11.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d4croa_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████████████████▏                                     | 7089/12312 [15:11<12:03,  7.22it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 58%|███████████████████████████████████████████████████▎                                     | 7091/12312 [15:11<09:30,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1c53a_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████████████████████████████████████████▎                        | 8890/12312 [19:00<06:50,  8.33it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 72%|████████████████████████████████████████████████████████████████▎                        | 8892/12312 [19:00<05:52,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d2zjq51.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████████████████▍                   | 9601/12312 [20:32<06:34,  6.86it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 78%|█████████████████████████████████████████████████████████████████████▍                   | 9604/12312 [20:32<05:15,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1ffkw_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████████████████████▍               | 10134/12312 [21:42<05:06,  7.10it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 82%|████████████████████████████████████████████████████████████████████████▍               | 10136/12312 [21:42<03:56,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d2zjr11.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████████▋               | 10173/12312 [21:46<03:34,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1so7a_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|████████████████████████████████████████████████████████████████████████████████▏       | 11217/12312 [23:55<02:27,  7.43it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 91%|████████████████████████████████████████████████████████████████████████████████▏       | 11219/12312 [23:55<02:00,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/training/d1aina_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 12312/12312 [26:10<00:00,  7.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_path = Path(\"/data/hongyiling/XAI4e3nn/dataset/scope/training\")\n",
    "\n",
    "all_files = list(root_path.rglob('*.hdf5'))\n",
    "\n",
    "all_files = [str(file) for file in all_files if file.is_file()]\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    try:\n",
    "        structure = create_structure_from_hdf5(file)\n",
    "        residue_num = len([1 for _ in structure.get_residues()])\n",
    "        \n",
    "        dssp_executable_path = 'mkdssp'\n",
    "        output_pdb_file_name = 'output_structure.pdb'\n",
    "\n",
    "        write_structure_to_pdb(structure, output_pdb_file_name)\n",
    "\n",
    "        # Run DSSP analysis\n",
    "        label = assign_secondary_structure_with_dssp(output_pdb_file_name, dssp_executable_path, residue_num = residue_num)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        assert(len(label) == residue_num)\n",
    "\n",
    "        output_file_name = file[:-5]\n",
    "    #         print(output_file_name)\n",
    "        np.save(output_file_name, label)\n",
    "        \n",
    "    except:\n",
    "        print(file)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d749184d-954b-4496-ac38-a545fe69a14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████████▌                                                        | 284/736 [00:34<01:05,  6.91it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 39%|████████████████████████████████████                                                        | 288/736 [00:34<00:41, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/validation/d1i96v_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████████████████████████████████████▏                                     | 433/736 [00:52<00:35,  8.59it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 59%|██████████████████████████████████████████████████████▋                                     | 437/736 [00:53<00:26, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/validation/d1ffki_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████████████████████████████████████▋     | 693/736 [01:28<00:07,  5.48it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 94%|██████████████████████████████████████████████████████████████████████████████████████▉     | 695/736 [01:28<00:05,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/validation/d1mlia_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 736/736 [01:33<00:00,  7.88it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_path = Path(\"/data/hongyiling/XAI4e3nn/dataset/scope/validation\")\n",
    "\n",
    "all_files = list(root_path.rglob('*.hdf5'))\n",
    "\n",
    "all_files = [str(file) for file in all_files if file.is_file()]\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    try:\n",
    "        structure = create_structure_from_hdf5(file)\n",
    "        residue_num = len([1 for _ in structure.get_residues()])\n",
    "        \n",
    "        dssp_executable_path = 'mkdssp'\n",
    "        output_pdb_file_name = 'output_structure.pdb'\n",
    "\n",
    "        write_structure_to_pdb(structure, output_pdb_file_name)\n",
    "\n",
    "        # Run DSSP analysis\n",
    "        label = assign_secondary_structure_with_dssp(output_pdb_file_name, dssp_executable_path, residue_num = residue_num)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        assert(len(label) == residue_num)\n",
    "\n",
    "        output_file_name = file[:-5]\n",
    "    #         print(output_file_name)\n",
    "        np.save(output_file_name, label)\n",
    "        \n",
    "    except:\n",
    "        print(file)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3301a7a7-e3af-4cf7-9f49-1cd02c0d4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████████████▊                                                                          | 235/1272 [00:28<02:09,  7.98it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 19%|████████████████▉                                                                          | 237/1272 [00:28<01:50,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/test_family/d2at2a2.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████▍                                                                        | 257/1272 [00:30<02:08,  7.87it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 20%|██████████████████▌                                                                        | 259/1272 [00:30<01:44,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/test_family/d1f6ga_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████████████████████████████████▉                                     | 754/1272 [01:31<00:53,  9.70it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/test_family/d1cpb.1.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████████████████████████████████████████▋          | 1127/1272 [02:17<00:25,  5.78it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 89%|███████████████████████████████████████████████████████████████████████████████▉          | 1129/1272 [02:17<00:18,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/test_family/d1f1oa_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1272/1272 [02:35<00:00,  8.17it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_path = Path(\"/data/hongyiling/XAI4e3nn/dataset/scope/test_family\")\n",
    "\n",
    "all_files = list(root_path.rglob('*.hdf5'))\n",
    "\n",
    "all_files = [str(file) for file in all_files if file.is_file()]\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    try:\n",
    "        structure = create_structure_from_hdf5(file)\n",
    "        residue_num = len([1 for _ in structure.get_residues()])\n",
    "        \n",
    "        dssp_executable_path = 'mkdssp'\n",
    "        output_pdb_file_name = 'output_structure.pdb'\n",
    "\n",
    "        write_structure_to_pdb(structure, output_pdb_file_name)\n",
    "\n",
    "        # Run DSSP analysis\n",
    "        label = assign_secondary_structure_with_dssp(output_pdb_file_name, dssp_executable_path, residue_num = residue_num)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        assert(len(label) == residue_num)\n",
    "\n",
    "        output_file_name = file[:-5]\n",
    "    #         print(output_file_name)\n",
    "        np.save(output_file_name, label)\n",
    "        \n",
    "    except:\n",
    "        print(file)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff515fbe-aa5d-4a5f-9dc3-a111b71409a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 718/718 [01:23<00:00,  8.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_path = Path(\"/data/hongyiling/XAI4e3nn/dataset/scope/test_fold\")\n",
    "\n",
    "all_files = list(root_path.rglob('*.hdf5'))\n",
    "\n",
    "all_files = [str(file) for file in all_files if file.is_file()]\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    try:\n",
    "        structure = create_structure_from_hdf5(file)\n",
    "        residue_num = len([1 for _ in structure.get_residues()])\n",
    "        \n",
    "        dssp_executable_path = 'mkdssp'\n",
    "        output_pdb_file_name = 'output_structure.pdb'\n",
    "\n",
    "        write_structure_to_pdb(structure, output_pdb_file_name)\n",
    "\n",
    "        # Run DSSP analysis\n",
    "        label = assign_secondary_structure_with_dssp(output_pdb_file_name, dssp_executable_path, residue_num = residue_num)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        assert(len(label) == residue_num)\n",
    "\n",
    "        output_file_name = file[:-5]\n",
    "    #         print(output_file_name)\n",
    "        np.save(output_file_name, label)\n",
    "        \n",
    "    except:\n",
    "        print(file)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e0a31b6-4c0f-46fe-9946-f04c5e41a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████▉                                                                          | 234/1254 [00:30<01:58,  8.63it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 19%|█████████████████▏                                                                         | 236/1254 [00:30<01:38, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/test_superfamily/d1lepa_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████████▌                                                            | 422/1254 [00:57<02:06,  6.60it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 34%|██████████████████████████████▊                                                            | 424/1254 [00:57<01:42,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/test_superfamily/d1rh2a_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████████▊                                      | 727/1254 [01:39<01:29,  5.91it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 58%|████████████████████████████████████████████████████▉                                      | 729/1254 [01:39<01:04,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/test_superfamily/d2riga_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████████████████████████████████████████▊          | 1112/1254 [02:30<00:18,  7.85it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 89%|████████████████████████████████████████████████████████████████████████████████          | 1116/1254 [02:31<00:15,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/test_superfamily/d1hmca_.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████████████▏   | 1201/1254 [02:41<00:07,  6.82it/s]/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n",
      " 96%|██████████████████████████████████████████████████████████████████████████████████████▍   | 1205/1254 [02:42<00:04, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/XAI4e3nn/dataset/scope/test_superfamily/d1qcrd2.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1254/1254 [02:48<00:00,  7.44it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_path = Path(\"/data/hongyiling/XAI4e3nn/dataset/scope/test_superfamily\")\n",
    "\n",
    "all_files = list(root_path.rglob('*.hdf5'))\n",
    "\n",
    "all_files = [str(file) for file in all_files if file.is_file()]\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    try:\n",
    "        structure = create_structure_from_hdf5(file)\n",
    "        residue_num = len([1 for _ in structure.get_residues()])\n",
    "        \n",
    "        dssp_executable_path = 'mkdssp'\n",
    "        output_pdb_file_name = 'output_structure.pdb'\n",
    "\n",
    "        write_structure_to_pdb(structure, output_pdb_file_name)\n",
    "\n",
    "        # Run DSSP analysis\n",
    "        label = assign_secondary_structure_with_dssp(output_pdb_file_name, dssp_executable_path, residue_num = residue_num)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        assert(len(label) == residue_num)\n",
    "\n",
    "        output_file_name = file[:-5]\n",
    "    #         print(output_file_name)\n",
    "        np.save(output_file_name, label)\n",
    "        \n",
    "    except:\n",
    "        print(file)\n",
    "\n",
    "       \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba0892-ec45-4383-937b-f7e796506b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19d2de-8714-4c5f-9d0b-ad2a17956b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3c1e3d1-9b78-4801-bbf3-433c4ed979f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('a') - ord('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ea5bc43-3c3c-492d-9369-52487cd8360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "classes_ = {}\n",
    "with open(\"/data/hongyiling/XAI4e3nn/dataset/scope/class_map.txt\", 'r') as mFile:\n",
    "    for line in mFile:\n",
    "        lineList = line.rstrip().split('\\t')\n",
    "        classes_[lineList[0]] = ord(lineList[0][0]) - ord('a')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0c39269-04fe-46b6-a462-acead86fed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hdf5_file_path = '/data/hongyiling/XAI4e3nn/dataset/scope/training/d1sctb_.hdf5'\n",
    "h5File = h5py.File(hdf5_file_path, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83049c95-3ae0-47c4-be81-69db468a6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "    # Assuming the HDF5 file has datasets for coordinates, atom names, residue names, chain IDs, and residue numbers\n",
    "    # The actual implementation may need adjustments based on the HDF5 file structure\n",
    "    coordinates = hdf5_file['atom_pos'][0,:]\n",
    "    atom_names = hdf5_file['atom_names'][:]\n",
    "    residue_names = hdf5_file['atom_residue_names'][:]\n",
    "    chain_ids = hdf5_file['atom_chain_ids'][:]\n",
    "    residue_nums = hdf5_file['atom_residue_id'][:]\n",
    "    atom_types = hdf5_file['atom_types'][:]\n",
    "    amino_types = hdf5_file['amino_types'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7722de5-dd0e-44fa-b293-e33d1aa9cb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"amino_types\": shape (150,), type \"<i8\">"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5File['amino_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c388db53-f521-43ca-9164-f2a60e464140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"atom_pos\": shape (1, 1145, 3), type \"<f8\">"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5File[\"atom_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "472b6bd2-75b2-4612-91ef-f7de72de367b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  9, 11,  9, 15,  0,  9, 11,  4,  9,  2,  2, 16,  9, 16, 15,  6,\n",
       "       15,  6,  9, 19, 11,  4,  3,  9, 13,  6,  9,  2,  0,  7, 12, 15,  6,\n",
       "        9, 13,  6,  1,  8, 16,  2,  6,  9,  6,  9,  6,  3, 19, 10,  4,  9,\n",
       "        4,  8, 17,  4,  2, 11,  9, 10, 11, 10, 18,  2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4de9373f-0b06-423d-b98d-bc1686c3d599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77a9bd0f-8055-4957-bf8a-63383782a55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'PRO', b'LYS', b'MET', b'LYS', b'THR', b'HIS', b'LYS', b'MET',\n",
       "       b'ALA', b'LYS', b'ARG', b'ARG', b'ILE', b'LYS', b'ILE', b'THR',\n",
       "       b'GLY', b'THR', b'GLY', b'LYS', b'VAL', b'MET', b'ALA', b'PHE',\n",
       "       b'LYS', b'SER', b'GLY', b'LYS', b'ARG', b'HIS', b'GLN', b'ASN',\n",
       "       b'THR', b'GLY', b'LYS', b'SER', b'GLY', b'ASP', b'GLU', b'ILE',\n",
       "       b'ARG', b'GLY', b'LYS', b'GLY', b'LYS', b'GLY', b'PHE', b'VAL',\n",
       "       b'LEU', b'ALA', b'LYS', b'ALA', b'GLU', b'TRP', b'ALA', b'ARG',\n",
       "       b'MET', b'LYS', b'LEU', b'MET', b'LEU', b'PRO', b'ARG'],\n",
       "      dtype='|S3')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residue_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e11372e-7a1b-4984-a1a2-6ca9a39cf9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae75cf39-e32f-4501-9903-bff86bead2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask_n = np.char.equal(atom_names, b'N')\n",
    "mask_ca = np.char.equal(atom_names, b'CA')\n",
    "mask_c = np.char.equal(atom_names, b'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68be2388-4d05-42f6-a747-b27eb588e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ma.mask_or(np.ma.mask_or(mask_n, mask_ca), mask_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd655b35-1718-47cd-ae13-15fd1f9bf148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02ae5835-61c4-4d20-9124-e4c2c32ff976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_n.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb852d5f-3a4a-442e-bd75-86b11c19c3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/hongyiling/XAI4e3nn/dataset/scope/training/d1sctb_.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf5_file_path[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a04e54d-5fbf-4b2c-84d8-53afa7a1180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_label = np.load(hdf5_file_path[:-4] + \"npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf2ceec3-9423-450a-83f0-a9e14011a340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14666666666666667"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xai_label == 'None').sum() / len(xai_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0a2da35-507e-432a-816a-f70725db18c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xai_label[residue_idx]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e016e564-83b2-4fef-9094-e967dd43ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "residue_idx = residue_nums[mask]\n",
    "xai_labels_raw = xai_label[residue_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fab0592-f812-439b-801f-d65278a7af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_labels_proccessed = np.zeros(xai_labels_raw.shape)\n",
    "# xai_labels_proccessed\n",
    "xai_labels_proccessed[xai_labels_raw == 'alpha'] = 1.0\n",
    "xai_labels_proccessed[xai_labels_raw == 'beta'] = 1.0       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b9237-6d94-4614-8920-26d2977907d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eab97a8-b4f4-4e3a-97dc-a33b45f91cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.scope.dataloader import FOLDdataset, FOLDBackBonedataset\n",
    "\n",
    "from torch_geometric.transforms import RadiusGraph, KNNGraph\n",
    "\n",
    "max_radius = 5\n",
    "Trans = RadiusGraph(r = max_radius)\n",
    "# Trans = KNNGraph(k=5, loop=True)\n",
    "# dataset = FOLDdataset(root='/data/hongyiling/XAI4e3nn/dataset/scope/', split=\"training\", transform = Trans)\n",
    "dataset = FOLDBackBonedataset(root='/data/hongyiling/XAI4e3nn/dataset/scope/', split=\"training\", transform = Trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fce8def-1c8d-43f9-af14-0e2bd1fcb3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[438, 1], pos=[438, 3], xai_labels_raw=[438, 1], xai_labels=[438, 1, 1], id='d1v4wb_', y=[1], edge_index=[2, 6018])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a133810c-0fc8-47f0-8c1a-ecb994501021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-1].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea031d3-eadf-4a53-bdb7-9cf40feb6068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e881bb-8d59-4921-98d3-4b9bda6e075a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9614861e-5656-4383-aed1-0ec1723d6f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.67258382642998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6932 / 507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2a32d1-7c30-44b0-b123-7a204792ef6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.73972602739726"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6018 / 438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4ff027b-acee-4a25-a7ae-a2dc5ea6ad7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max atom type\n",
    "max([d.x.max() for d in dataset])\n",
    "# dataset[5].x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e886ba7d-906b-4846-b7cb-6bd31d2cad7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.826771653543307"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26091 / 1143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558262f5-372e-43f3-9751-f3304d53a192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a484dc-0dda-41e8-a88c-d8738835dcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a08697-4a15-4dbe-baeb-3bb84eb02e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_cluster import radius_graph\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_scatter import scatter\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.nn import FullyConnectedNet, Gate, NormActivation\n",
    "from e3nn.o3 import FullyConnectedTensorProduct, Linear\n",
    "from e3nn.math import soft_one_hot_linspace\n",
    "from e3nn.util.test import assert_equivariant\n",
    "\n",
    "from model.LRP import *\n",
    "\n",
    "class Convolution(torch.nn.Module):\n",
    "    def __init__(self, irreps_in, irreps_sh, irreps_out, num_neighbors) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_neighbors = num_neighbors\n",
    "\n",
    "        tp = FullyConnectedTensorProduct(\n",
    "            irreps_in1=irreps_in,\n",
    "            irreps_in2=irreps_sh,\n",
    "            irreps_out=irreps_out,\n",
    "            internal_weights=False,\n",
    "            shared_weights=False,\n",
    "        )\n",
    "        self.fc = FullyConnectedNet([5, 256, tp.weight_numel], torch.relu)\n",
    "        self.tp = tp\n",
    "        self.irreps_out = self.tp.irreps_out\n",
    "\n",
    "    def forward(self, node_features, edge_src, edge_dst, edge_attr, edge_scalars, edge_attn = None) -> torch.Tensor:\n",
    "        weight = self.fc(edge_scalars)\n",
    "        edge_features = self.tp(node_features[edge_src], edge_attr, weight)\n",
    "        \n",
    "        if edge_attn is not None: \n",
    "            edge_features = edge_features * edge_attn\n",
    "            \n",
    "        node_features = scatter(edge_features, edge_dst, dim=0).div(self.num_neighbors**0.5)\n",
    "        return node_features\n",
    "    \n",
    "    def FirstOrderX(self, R, node_features, edge_src, edge_dst, edge_attr, edge_scalars, zero_point = False):\n",
    "        weight = self.fc(edge_scalars)\n",
    "        edge_features = self.tp(node_features[edge_src], edge_attr, weight)\n",
    "\n",
    "        R = FirstOrder4Scatter(R, edge_dst = edge_dst, edge_features = edge_features)\n",
    "        R = FirstOrder4FullyConnectedTensorProduct(self.tp, R, node_features,  edge_src, edge_dst, edge_attr, weight, zero_point = zero_point)\n",
    "        return R\n",
    "\n",
    "\n",
    "class TFN(torch.nn.Module):\n",
    "    def __init__(self, l = 3) -> None:\n",
    "        super().__init__()\n",
    "        self.num_neighbors = 13.67 # typical number of neighbors\n",
    "        self.irreps_sh = o3.Irreps.spherical_harmonics(l)\n",
    "\n",
    "        irreps = self.irreps_sh \n",
    "\n",
    "        if (l == 3):\n",
    "            hidden_irreps = o3.Irreps(\"8x0o+8x0e+8x1o+8x1e+8x2o+8x2e+8x3o+8x3e\")\n",
    "        elif (l==2):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\")\n",
    "        elif (l==4):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e+16x3o+16x3e+16x4o+16x4e\")\n",
    "        elif (l==8):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e+16x3o+16x3e+16x4o+16x4e+16x5o+16x5e+16x6o+16x6e+16x7o+16x7e+16x8o+16x8e\")\n",
    "        elif (l==11):\n",
    "            hidden_irreps = o3.Irreps(\"8x0o+8x0e+8x1o+8x1e+8x2o+8x2e+8x3o+8x3e+8x4o+8x4e+8x5o+8x5e+8x6o+8x6e+8x7o+8x7e+8x8o+8x8e+8x9o+8x9e+8x10o+8x10e+8x11o+8x11e\")#+8x13o+8x13e+8x14o+8x14e+8x15o+8x15e+8x16o+8x16e\")\n",
    "\n",
    "\n",
    "        self.hidden_irreps = hidden_irreps\n",
    "\n",
    "        irreps_input = o3.Irreps(\"17x0e\") + self.irreps_sh \n",
    "        irreps = irreps_input\n",
    "        \n",
    "        normAct = NormActivation(hidden_irreps, torch.sigmoid)\n",
    "        \n",
    "        self.conv1 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.act = normAct\n",
    "        irreps = self.act.irreps_out\n",
    "        self.lin1 = Linear(irreps, irreps)\n",
    "        self.conv2 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.lin2 = Linear(irreps, irreps)\n",
    "        self.conv3 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.lin3 = Linear(irreps, irreps)\n",
    "        # Final layer\n",
    "        self.final = Convolution(irreps, self.irreps_sh, \"7x0e\", self.num_neighbors)\n",
    "        self.irreps_out = self.final.irreps_out\n",
    "\n",
    "    def forward(self, data, mask_order = []) -> torch.Tensor:\n",
    "        num_nodes = 13.67  # typical number of nodes\n",
    "\n",
    "        edge_src, edge_dst = data.edge_index\n",
    "        edge_vec = data.pos[edge_src] - data.pos[edge_dst]\n",
    "        edge_vec = edge_vec.float()\n",
    "        edge_attr = o3.spherical_harmonics(l=self.irreps_sh, x=edge_vec, normalize=True, normalization=\"component\")\n",
    "\n",
    "        for i in mask_order:\n",
    "            edge_attr[:, self.irreps_sh.slices()[i] ] = 0.0\n",
    "\n",
    "        edge_length_embedded = (\n",
    "            soft_one_hot_linspace(x=edge_vec.norm(dim=1), start=0.0, end=max_radius, number=5, basis=\"smooth_finite\", cutoff=True)\n",
    "            * 5**0.5\n",
    "        )\n",
    "\n",
    "        if \"edge_attn\" in data:\n",
    "            edge_attn = data.edge_attn\n",
    "            edge_attr = edge_attr * edge_attn\n",
    "        else:\n",
    "            edge_attn = None\n",
    "\n",
    "        x = scatter(edge_attr, edge_dst, dim=0).div(self.num_neighbors**0.5)\n",
    "\n",
    "#         x = torch.cat( [F.one_hot(data.x, num_classes = 16).squeeze(), x], dim = 1 )\n",
    "        x = torch.cat([data.x, x], dim = 1)\n",
    "\n",
    "        x = self.conv1(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "        \n",
    "        x = self.conv2(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "            \n",
    "        x = self.conv3(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "            \n",
    "        x = self.final(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "\n",
    "        return scatter(x, data.batch, dim=0).div(num_nodes**0.5)\n",
    "\n",
    "    def Grad_CAM(self, data):\n",
    "\n",
    "        num_nodes = 13.67   # typical number of nodes\n",
    "\n",
    "        edge_src, edge_dst = data.edge_index\n",
    "        edge_vec = data.pos[edge_src] - data.pos[edge_dst]\n",
    "        edge_attr = o3.spherical_harmonics(l=self.irreps_sh, x=edge_vec, normalize=True, normalization=\"component\")\n",
    "\n",
    "        edge_length_embedded = (\n",
    "            soft_one_hot_linspace(x=edge_vec.norm(dim=1), start=0.0, end=max_radius, number=5, basis=\"smooth_finite\", cutoff=True)\n",
    "            * 5**0.5\n",
    "        )\n",
    "\n",
    "        x = scatter(edge_attr, edge_dst, dim=0).div(self.num_neighbors**0.5)\n",
    "#         x = torch.cat( [F.one_hot(data.x, num_classes = 16).squeeze(), x], dim = 1 )\n",
    "        x = torch.cat([data.x, x], dim = 1)\n",
    "        act1 = x\n",
    "\n",
    "        # Forward\n",
    "        x = self.conv1(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        act2 = x\n",
    "\n",
    "        x = self.conv2(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        act3 = x\n",
    "        x = self.conv3(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        act4 = x\n",
    "        \n",
    "        x = self.final(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        act5 = x\n",
    "        x = scatter(x, data.batch, dim=0).div(num_nodes**0.5)\n",
    "\n",
    "        # Gradient\n",
    "        act1.retain_grad()\n",
    "        act2.retain_grad()\n",
    "        act3.retain_grad()\n",
    "        act4.retain_grad()\n",
    "        act5.retain_grad()\n",
    "        \n",
    "        ### compute gradient using label class\n",
    "        self.zero_grad()\n",
    "        x[0, data.y].backward(retain_graph=True)\n",
    "#         x.sum().backward(retain_graph=True)\n",
    "        \n",
    "        return [act1.grad * act1, act2.grad * act2, act3.grad * act3, act4.grad * act4, act5.grad * act5]\n",
    "\n",
    "    def FirstOrderX(self, data, zero_point = False, R = None, Norm = True):\n",
    "        num_nodes = 13.67   # typical number of nodes\n",
    "\n",
    "        edge_src, edge_dst = data.edge_index\n",
    "        edge_vec = data.pos[edge_src] - data.pos[edge_dst]\n",
    "        edge_attr = o3.spherical_harmonics(l=self.irreps_sh, x=edge_vec, normalize=True, normalization=\"component\")\n",
    "\n",
    "        edge_length_embedded = (\n",
    "            soft_one_hot_linspace(x=edge_vec.norm(dim=1), start=0.0, end=max_radius, number=5, basis=\"smooth_finite\", cutoff=True)\n",
    "            * 5**0.5\n",
    "        )\n",
    "\n",
    "        ##Forward\n",
    "        Xs = []\n",
    "        x = scatter(edge_attr, edge_dst, dim=0).div(self.num_neighbors**0.5)\n",
    "#         x = torch.cat( [F.one_hot(data.x, num_classes = 16).squeeze(), x], dim = 1 )\n",
    "        x = torch.cat([data.x, x], dim = 1)\n",
    "        \n",
    "        Xs.append(x)\n",
    "        x = self.conv1(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        Xs.append(x)\n",
    "        x = self.lin1(x)\n",
    "        Xs.append(x)\n",
    "        x = self.act(x)\n",
    "        Xs.append(x)\n",
    "\n",
    "        x = self.conv2(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        Xs.append(x)\n",
    "        x = self.lin2(x)\n",
    "        Xs.append(x)\n",
    "        x = self.act(x)\n",
    "        Xs.append(x)\n",
    "\n",
    "        x = self.conv3(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        Xs.append(x)\n",
    "        x = self.lin3(x)\n",
    "        Xs.append(x)\n",
    "        x = self.act(x)\n",
    "        Xs.append(x)\n",
    "        \n",
    "        x = self.final(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        Xs.append(x)\n",
    "        x = scatter(x, data.batch, dim=0).div(num_nodes**0.5)\n",
    "        Xs.append(x)\n",
    "\n",
    "        Rs = []\n",
    "        if R is None:\n",
    "            R = x.clone().detach()\n",
    "        R = FirstOrder4Scatter(R, data.batch, edge_features = Xs[-2])\n",
    "        Rs.append(R)\n",
    "\n",
    "        R = self.final.FirstOrderX(R, Xs[9], edge_src, edge_dst, edge_attr, edge_length_embedded, zero_point = zero_point)\n",
    "        Rs.append(R)\n",
    "\n",
    "        if (Norm):\n",
    "            R = FirstOrder4NormActivation(self.act, Xs[8], R, irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\"))\n",
    "        else:\n",
    "            R = R\n",
    "        Rs.append(R)\n",
    "\n",
    "        R = FirstOrder4Linear(R, self.lin3, Xs[7])\n",
    "        Rs.append(R)\n",
    "\n",
    "        R = self.conv3.FirstOrderX(R, Xs[6], edge_src, edge_dst, edge_attr, edge_length_embedded, zero_point = zero_point)\n",
    "        Rs.append(R)\n",
    "\n",
    "        if (Norm):\n",
    "            R = FirstOrder4NormActivation(self.act, Xs[5], R, irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\"))\n",
    "        else:\n",
    "            R = R\n",
    "        Rs.append(R)\n",
    "\n",
    "        R = FirstOrder4Linear(R, self.lin2, Xs[4])\n",
    "        Rs.append(R)\n",
    "\n",
    "        R = self.conv2.FirstOrderX(R, Xs[3], edge_src, edge_dst, edge_attr, edge_length_embedded, zero_point = zero_point)\n",
    "        Rs.append(R)\n",
    "\n",
    "        if (Norm):\n",
    "            R = FirstOrder4NormActivation(self.act, Xs[2], R, irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\"))\n",
    "        else:\n",
    "            R = R\n",
    "        Rs.append(R)\n",
    "        \n",
    "        R = FirstOrder4Linear(R, self.lin1, Xs[1])\n",
    "        Rs.append(R)\n",
    "        \n",
    "        R = self.conv1.FirstOrderX(R, Xs[0], edge_src, edge_dst, edge_attr, edge_length_embedded, zero_point = zero_point)\n",
    "        Rs.append(R)\n",
    "\n",
    "        # R = FirstOrder4Scatter(R, edge_dst = edge_dst, edge_features = edge_attr)\n",
    "        # Rs.append(R)\n",
    "\n",
    "        return Rs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e0415-1661-4d23-98ee-6afa7f0095b5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76710077-8d38-43d5-a5c9-8f7d029c4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73391cdc-e43d-4407-89d4-2b61e4e525fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_cluster import radius_graph\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_scatter import scatter\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.nn import FullyConnectedNet, Gate, NormActivation\n",
    "from e3nn.o3 import FullyConnectedTensorProduct, Linear\n",
    "from e3nn.math import soft_one_hot_linspace\n",
    "from e3nn.util.test import assert_equivariant\n",
    "\n",
    "from LRP import *\n",
    "\n",
    "class Convolution(torch.nn.Module):\n",
    "    def __init__(self, irreps_in, irreps_sh, irreps_out, num_neighbors) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_neighbors = num_neighbors\n",
    "\n",
    "        tp = FullyConnectedTensorProduct(\n",
    "            irreps_in1=irreps_in,\n",
    "            irreps_in2=irreps_sh,\n",
    "            irreps_out=irreps_out,\n",
    "            internal_weights=False,\n",
    "            shared_weights=False,\n",
    "        )\n",
    "        self.fc = FullyConnectedNet([5, 256, tp.weight_numel], torch.relu)\n",
    "        self.tp = tp\n",
    "        self.irreps_out = self.tp.irreps_out\n",
    "\n",
    "    def forward(self, node_features, edge_src, edge_dst, edge_attr, edge_scalars, edge_attn = None) -> torch.Tensor:\n",
    "        weight = self.fc(edge_scalars)\n",
    "        edge_features = self.tp(node_features[edge_src], edge_attr, weight)\n",
    "        \n",
    "        if edge_attn is not None: \n",
    "            edge_features = edge_features * edge_attn\n",
    "            \n",
    "        node_features = scatter(edge_features, edge_dst, dim=0).div(self.num_neighbors**0.5)\n",
    "        return node_features\n",
    "    \n",
    "    def FirstOrderX(self, R, node_features, edge_src, edge_dst, edge_attr, edge_scalars, zero_point = False):\n",
    "        weight = self.fc(edge_scalars)\n",
    "        edge_features = self.tp(node_features[edge_src], edge_attr, weight)\n",
    "\n",
    "        R = FirstOrder4Scatter(R, edge_dst = edge_dst, edge_features = edge_features)\n",
    "        R, R_edgeAttr, R_edgeDist = FirstOrder4FullyConnectedTensorProductWithEdgeOutput(self.tp, R, node_features,  edge_src, edge_dst, edge_attr, weight, zero_point = zero_point)\n",
    "        return R, R_edgeAttr, R_edgeDist.sum(-1)\n",
    "\n",
    "\n",
    "class TFN(torch.nn.Module):\n",
    "    def __init__(self, l = 3) -> None:\n",
    "        super().__init__()\n",
    "        self.num_neighbors = 13.67 # typical number of neighbors\n",
    "        self.irreps_sh = o3.Irreps.spherical_harmonics(l)\n",
    "\n",
    "        irreps = self.irreps_sh \n",
    "\n",
    "        if (l == 3):\n",
    "            hidden_irreps = o3.Irreps(\"8x0o+8x0e+8x1o+8x1e+8x2o+8x2e+8x3o+8x3e\")\n",
    "        elif (l==2):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\")\n",
    "        elif (l==4):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e+16x3o+16x3e+16x4o+16x4e\")\n",
    "        elif (l==8):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e+16x3o+16x3e+16x4o+16x4e+16x5o+16x5e+16x6o+16x6e+16x7o+16x7e+16x8o+16x8e\")\n",
    "        elif (l==11):\n",
    "            hidden_irreps = o3.Irreps(\"8x0o+8x0e+8x1o+8x1e+8x2o+8x2e+8x3o+8x3e+8x4o+8x4e+8x5o+8x5e+8x6o+8x6e+8x7o+8x7e+8x8o+8x8e+8x9o+8x9e+8x10o+8x10e+8x11o+8x11e\")#+8x13o+8x13e+8x14o+8x14e+8x15o+8x15e+8x16o+8x16e\")\n",
    "\n",
    "\n",
    "        self.hidden_irreps = hidden_irreps\n",
    "\n",
    "        irreps_input = o3.Irreps(\"17x0e\") + self.irreps_sh \n",
    "        irreps = irreps_input\n",
    "        \n",
    "        normAct = NormActivation(hidden_irreps, torch.sigmoid)\n",
    "        \n",
    "        self.conv1 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.act = normAct\n",
    "        irreps = self.act.irreps_out\n",
    "        self.lin1 = Linear(irreps, irreps)\n",
    "        self.conv2 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.lin2 = Linear(irreps, irreps)\n",
    "        self.conv3 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.lin3 = Linear(irreps, irreps)\n",
    "        # Final layer\n",
    "        self.final = Convolution(irreps, self.irreps_sh, \"7x0e\", self.num_neighbors)\n",
    "        self.irreps_out = self.final.irreps_out\n",
    "\n",
    "    def forward(self, data, mask_order = [], mask_node = None) -> torch.Tensor:\n",
    "        num_nodes = 13.67  # typical number of nodes\n",
    "\n",
    "        edge_src, edge_dst = data.edge_index\n",
    "        edge_vec = data.pos[edge_src] - data.pos[edge_dst]\n",
    "        edge_vec = edge_vec.float()\n",
    "        edge_attr = o3.spherical_harmonics(l=self.irreps_sh, x=edge_vec, normalize=True, normalization=\"component\")\n",
    "\n",
    "        for i in mask_order:\n",
    "            edge_attr[:, self.irreps_sh.slices()[i] ] = 0.0\n",
    "\n",
    "        edge_length_embedded = (\n",
    "            soft_one_hot_linspace(x=edge_vec.norm(dim=1), start=0.0, end=max_radius, number=5, basis=\"smooth_finite\", cutoff=True)\n",
    "            * 5**0.5\n",
    "        )\n",
    "\n",
    "        if \"edge_attn\" in data:\n",
    "            edge_attn = data.edge_attn\n",
    "            edge_attr = edge_attr * edge_attn\n",
    "        else:\n",
    "            edge_attn = None\n",
    "\n",
    "        x = scatter(edge_attr, edge_dst, dim=0).div(self.num_neighbors**0.5)\n",
    "\n",
    "#         x = torch.cat( [F.one_hot(data.x, num_classes = 16).squeeze(), x], dim = 1 )\n",
    "        x = torch.cat([data.x, x], dim = 1)\n",
    "    \n",
    "        if mask_node is not None:\n",
    "            x[mask_node] = 0.0\n",
    "\n",
    "        x = self.conv1(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "        \n",
    "        if mask_node is not None:\n",
    "            x[mask_node] = 0.0\n",
    "        \n",
    "        x = self.conv2(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "            \n",
    "        if mask_node is not None:\n",
    "            x[mask_node] = 0.0\n",
    "            \n",
    "        x = self.conv3(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "            \n",
    "        if mask_node is not None:\n",
    "            x[mask_node] = 0.0\n",
    "            \n",
    "        x = self.final(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        \n",
    "        if mask_node is not None:\n",
    "            x[mask_node] = 0.0\n",
    "\n",
    "        return scatter(x, data.batch, dim=0).div(num_nodes**0.5)\n",
    "    \n",
    "    def FirstOrderX(self, data, zero_point = False, R = None, Norm = True):\n",
    "        num_nodes = 13.67   # typical number of nodes\n",
    "\n",
    "        edge_src, edge_dst = data.edge_index\n",
    "        edge_vec = data.pos[edge_src] - data.pos[edge_dst]\n",
    "        edge_attr = o3.spherical_harmonics(l=self.irreps_sh, x=edge_vec, normalize=True, normalization=\"component\")\n",
    "\n",
    "        edge_length_embedded = (\n",
    "            soft_one_hot_linspace(x=edge_vec.norm(dim=1), start=0.0, end=max_radius, number=5, basis=\"smooth_finite\", cutoff=True)\n",
    "            * 5**0.5\n",
    "        )\n",
    "\n",
    "        ##Forward\n",
    "        Xs = []\n",
    "        x = scatter(edge_attr, edge_dst, dim=0).div(self.num_neighbors**0.5)\n",
    "#         x = torch.cat( [F.one_hot(data.x, num_classes = 16).squeeze(), x], dim = 1 )\n",
    "        x = torch.cat([data.x, x], dim = 1)\n",
    "        \n",
    "        Xs.append(x)\n",
    "        x = self.conv1(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        Xs.append(x)\n",
    "        x = self.lin1(x)\n",
    "        Xs.append(x)\n",
    "        x = self.act(x)\n",
    "        Xs.append(x)\n",
    "\n",
    "        x = self.conv2(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        Xs.append(x)\n",
    "        x = self.lin2(x)\n",
    "        Xs.append(x)\n",
    "        x = self.act(x)\n",
    "        Xs.append(x)\n",
    "\n",
    "        x = self.conv3(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        Xs.append(x)\n",
    "        x = self.lin3(x)\n",
    "        Xs.append(x)\n",
    "        x = self.act(x)\n",
    "        Xs.append(x)\n",
    "        \n",
    "        x = self.final(x, edge_src, edge_dst, edge_attr, edge_length_embedded)\n",
    "        Xs.append(x)\n",
    "        x = scatter(x, data.batch, dim=0).div(num_nodes**0.5)\n",
    "        Xs.append(x)\n",
    "\n",
    "        Rs = []\n",
    "        Rs_edgeAttr = []\n",
    "        Rs_edgeDist = []\n",
    "        \n",
    "        if R is None:\n",
    "            R = x.clone().detach()\n",
    "        R = FirstOrder4Scatter(R, data.batch, edge_features = Xs[-2])\n",
    "        Rs.append(R)\n",
    "\n",
    "        R, R_edgeAttr, R_edgeDist = self.final.FirstOrderX(R, Xs[9], edge_src, edge_dst, edge_attr, edge_length_embedded, zero_point = zero_point)\n",
    "        Rs.append(R)\n",
    "        Rs_edgeAttr.append(R_edgeAttr)\n",
    "        Rs_edgeDist.append(R_edgeDist)\n",
    "\n",
    "        if (Norm):\n",
    "            R = FirstOrder4NormActivation(self.act, Xs[8], R, irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\"))\n",
    "        else:\n",
    "            R = R\n",
    "        Rs.append(R)\n",
    "\n",
    "        R = FirstOrder4Linear(R, self.lin3, Xs[7])\n",
    "        Rs.append(R)\n",
    "\n",
    "        R, R_edgeAttr, R_edgeDist = self.conv3.FirstOrderX(R, Xs[6], edge_src, edge_dst, edge_attr, edge_length_embedded, zero_point = zero_point)\n",
    "        Rs.append(R)\n",
    "        Rs_edgeAttr.append(R_edgeAttr)\n",
    "        Rs_edgeDist.append(R_edgeDist)\n",
    "\n",
    "        if (Norm):\n",
    "            R = FirstOrder4NormActivation(self.act, Xs[5], R, irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\"))\n",
    "        else:\n",
    "            R = R\n",
    "        Rs.append(R)\n",
    "\n",
    "        R = FirstOrder4Linear(R, self.lin2, Xs[4])\n",
    "        Rs.append(R)\n",
    "\n",
    "        R, R_edgeAttr, R_edgeDist = self.conv2.FirstOrderX(R, Xs[3], edge_src, edge_dst, edge_attr, edge_length_embedded, zero_point = zero_point)\n",
    "        Rs.append(R)\n",
    "        Rs_edgeAttr.append(R_edgeAttr)\n",
    "        Rs_edgeDist.append(R_edgeDist)\n",
    "\n",
    "        if (Norm):\n",
    "            R = FirstOrder4NormActivation(self.act, Xs[2], R, irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\"))\n",
    "        else:\n",
    "            R = R\n",
    "        Rs.append(R)\n",
    "        \n",
    "        R = FirstOrder4Linear(R, self.lin1, Xs[1])\n",
    "        Rs.append(R)\n",
    "        \n",
    "        R, R_edgeAttr, R_edgeDist = self.conv1.FirstOrderX(R, Xs[0], edge_src, edge_dst, edge_attr, edge_length_embedded, zero_point = zero_point)\n",
    "        Rs.append(R)\n",
    "        Rs_edgeAttr.append(R_edgeAttr)\n",
    "        Rs_edgeDist.append(R_edgeDist)\n",
    "        \n",
    "        Rs.append(R[:,:17])\n",
    "        \n",
    "        R_edgeAttr = FirstOrder4Scatter(R[:,17:], edge_dst = edge_dst, edge_features = edge_attr)\n",
    "        Rs_edgeAttr.append(R_edgeAttr)\n",
    "        \n",
    "#         R = FirstOrder4Scatter(R, edge_dst = edge_dst, edge_features = edge_attr)\n",
    "#         Rs.append(R)\n",
    "\n",
    "        return Rs, Rs_edgeAttr, Rs_edgeDist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6db5aab2-e30f-4413-966c-62d7036adeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "cnt = defaultdict(int)\n",
    "for data in test_dataset:\n",
    "    cnt[data.y.item()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0af0a916-a744-46e1-9c3d-b70f0dc9cebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {0: 168, 1: 190, 2: 134, 3: 162, 5: 36, 6: 28})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a018889-f186-4d58-ae86-ba9020c7c7fb",
   "metadata": {},
   "source": [
    "# EquiGX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "084473f4-4163-473e-94da-291b58d00c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.scope.dataloader import FOLDBackBonedataset\n",
    "\n",
    "from torch_geometric.transforms import RadiusGraph\n",
    "\n",
    "max_radius = 5\n",
    "Trans = RadiusGraph(r = max_radius)\n",
    "dataset = FOLDBackBonedataset(root='./dataset/scope/', split=\"training\", transform = Trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef763fe-dcf8-4f0a-a3e1-df987e1087a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = FOLDBackBonedataset(root='./dataset/scope/', split=\"validation\", transform = Trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2554e5-7fc2-4532-9398-3b2466497800",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FOLDBackBonedataset(root='./dataset/scope/', split=\"test_fold\", transform = Trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b9a875-5002-4dd5-8b10-edc18d4ec8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fd310f7-cfdd-4737-8a62-7f7d612d428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI2/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 718/718 [04:23<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855549958820982 0.8559063226990326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "model = TFN(l=3)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3.pt\"))\n",
    "\n",
    "print(\"testing\")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n",
    "\n",
    "auroc_list = []\n",
    "ap_list = []\n",
    "\n",
    "\n",
    "for idx, test_batch in enumerate(tqdm(test_loader)):\n",
    "    if test_batch.y > 1:\n",
    "        continue\n",
    "    Rs, Rs_edgeAttr, Rs_edgeDist = model.FirstOrderX( test_batch.cuda(), R = F.one_hot(test_batch.y, num_classes = 7), zero_point = False, Norm = True)\n",
    "    \n",
    "    R_edgeAttr = torch.sum(torch.stack(Rs_edgeAttr), dim=0) \n",
    "#         R_edgeAttr += Rs[-1]\n",
    "    \n",
    "    R_edgeDist = torch.sum(torch.stack(Rs_edgeDist), dim=0) \n",
    "    \n",
    "    xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "    if (test_batch.y == 0):\n",
    "        xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "    elif (test_batch.y == 1):\n",
    "        xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "    node_importance_gt = xai_labels\n",
    "    \n",
    "    ### all \n",
    "    \n",
    "    node_importance_pred = (scatter(R_edgeAttr.sum(dim = 1) + R_edgeDist, test_batch.edge_index[0], dim = 0) + scatter(R_edgeAttr.sum(dim = 1) + R_edgeDist, test_batch.edge_index[1], dim = 0) ) / 2\n",
    "    node_importance_pred += Rs[-1].sum(dim = 1)\n",
    "    \n",
    "    try:\n",
    "        auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())\n",
    "        ap_score = average_precision_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())\n",
    "        \n",
    "        auroc_list.append(auroc_score)\n",
    "        ap_list.append(ap_score)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(np.mean(auroc_list), np.mean(ap_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "484b267e-6b87-4145-a4bf-225e05940fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(test_batch.cuda()).detach()\n",
    "torch.argmax(pred, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b97d7612-6f02-4198-a0ae-57a1bbd7e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 718/718 [09:03<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FO Norm results: avg AUROC: 0.5139237235372247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "fo_norm_results = []\n",
    "fo_norm_zp_results = []\n",
    "fo_zp_results = []\n",
    "fo_results = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    if (i>0):\n",
    "        continue\n",
    "    \n",
    "    # model = Network()\n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    # model.load_state_dict(torch.load(f\"./ckpt/synmol_simpleNet_{i}.pt\"))\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n",
    "\n",
    "    auroc_list = []\n",
    "    auroc_list_alpha = []\n",
    "    auroc_list_beta = []\n",
    "    auroc_list_alpha_beta = []\n",
    "    auroc_list_alpha_beta_ = []\n",
    "    \n",
    "    for idx, test_batch in enumerate(tqdm(test_loader)):\n",
    "        if test_batch.y > 3:\n",
    "            continue\n",
    "            \n",
    "        pred = model(test_batch.cuda()).detach()\n",
    "        \n",
    "        if (torch.argmax(pred, dim = 1)) != test_batch.y:\n",
    "            continue\n",
    "        \n",
    "        node_importance_gt = test_batch.xai_labels\n",
    "\n",
    "        Rs = model.FirstOrderX( test_batch.cuda(), zero_point = False, Norm = True)\n",
    "\n",
    "        # node_importance_pred = scatter(Rs[-1], test_batch.edge_index[0], dim = 0)\n",
    "        node_importance_pred = Rs[-1]\n",
    "        # node_importance_pred = Rs[-2]\n",
    "        node_importance_pred = node_importance_pred.sum(dim = 1)\n",
    "        \n",
    "#         xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "#         if (test_batch.y == 0):\n",
    "#             xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "#         elif (test_batch.y == 1):\n",
    "#             xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "        \n",
    "#         node_importance_gt = xai_labels\n",
    "        \n",
    "        try:\n",
    "            auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())\n",
    "            auroc_list.append(auroc_score)\n",
    "        except:\n",
    "            print(idx)\n",
    "        if test_batch.y == 0:\n",
    "            auroc_list_alpha.append(auroc_score)\n",
    "        elif test_batch.y == 1:\n",
    "            auroc_list_beta.append(auroc_score)\n",
    "        elif test_batch.y == 2:\n",
    "            auroc_list_alpha_beta.append(auroc_score)\n",
    "        elif test_batch.y == 3:\n",
    "            auroc_list_alpha_beta_.append(auroc_score)\n",
    "            \n",
    "    \n",
    "    print(f\"FO Norm results: avg AUROC: {np.mean(auroc_list)}\")\n",
    "    fo_norm_results.append( (np.mean(auroc_list) ))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea2a53ed-d502-45ac-82c2-d2415691dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4577225094368821"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auroc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2254ef86-2a7e-4eb5-bfae-bf1e591e2e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7072468260995632"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auroc_list_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cc3a938-4e94-4c34-87e8-41ff68f31ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22775354244000293"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auroc_list_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cff7bc-f969-49df-9883-e5b73ff942c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d1dc6-5bcd-46c6-9535-67a54b932c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f82fd34-d580-4b73-8d46-ac97a110793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [02:31<00:00,  4.73it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad results: avg AUROC: 0.5824613884028569\n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [02:39<00:00,  4.50it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad results: avg AUROC: 0.49725411663874575\n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [02:39<00:00,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad results: avg AUROC: 0.6140223799861416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grad_results = []\n",
    "cam_results = []\n",
    "\n",
    "grad_results_ap = []\n",
    "cam_results_ap = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "#     if (i == 1):\n",
    "#         continue\n",
    "    \n",
    "    # model = Network()\n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n",
    "\n",
    "    auroc_list = []\n",
    "    auroc_list_alpha = []\n",
    "    auroc_list_beta = []\n",
    "    auroc_list_alpha_beta = []\n",
    "    auroc_list_alpha_beta_ = []\n",
    "    \n",
    "    ap_list = [] \n",
    "    \n",
    "    for idx, test_batch in enumerate(tqdm(test_loader)):\n",
    "        if test_batch.y > 1:\n",
    "            continue\n",
    "        \n",
    "        node_importance_gt = test_batch.xai_labels\n",
    "\n",
    "        test_batch = test_batch.cuda()\n",
    "        test_batch.pos.requires_grad=True\n",
    "        output = model(test_batch)\n",
    "        model.zero_grad()\n",
    "        output[0, test_batch.y].backward()\n",
    "#         output.sum().backward()\n",
    "        node_importance_pred = test_batch.pos.grad.norm(dim = -1).detach().cpu()\n",
    "        \n",
    "        xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "        if (test_batch.y == 0):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "        elif (test_batch.y == 1):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "        \n",
    "        node_importance_gt = xai_labels\n",
    "        \n",
    "        try:\n",
    "            auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())\n",
    "            ap_score = average_precision_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())\n",
    "            \n",
    "            auroc_list.append(auroc_score)\n",
    "            ap_list.append(ap_score)\n",
    "            \n",
    "        except:\n",
    "            print(idx)\n",
    "        if test_batch.y == 0:\n",
    "            auroc_list_alpha.append(auroc_score)\n",
    "        elif test_batch.y == 1:\n",
    "            auroc_list_beta.append(auroc_score)\n",
    "        elif test_batch.y == 2:\n",
    "            auroc_list_alpha_beta.append(auroc_score)\n",
    "        elif test_batch.y == 3:\n",
    "            auroc_list_alpha_beta_.append(auroc_score)\n",
    "            \n",
    "    \n",
    "    print(f\"Grad results: avg AUROC: {np.mean(auroc_list)}\")\n",
    "    grad_results.append( (np.mean(auroc_list) ))\n",
    "    grad_results_ap.append( (np.mean(ap_list) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e0a445-3c1c-4d25-b76e-229f177b37d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.564579295009248, 0.04931891982728338)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(grad_results), np.std(grad_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baab56e2-0d8b-45d9-bf73-d010048898c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5975553438092596, 0.036819376508800954)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(grad_results_ap), np.std(grad_results_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af08f2-bbc0-4b4d-8294-cff3df8fc575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7ddc2-2c9e-426e-aff3-bdbf6bf0e1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ab55b34-51c0-4bc1-91f4-021e89ba565b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5982411975798415"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(grad_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef8c4ff-adbd-4654-a90c-d06d2d9fb702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5641565068411186"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l = 2\n",
    "np.mean(grad_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e10e4a-a1d7-42f6-bb5f-c114615883ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d8917-ed57-46bc-8fe6-824507a2ca74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f84d6fe8-3d06-4b61-9e29-7249cbe33dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [02:43<00:00,  4.40it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FO Norm results: avg AUROC: 0.6406163167902957\n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [02:47<00:00,  4.29it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FO Norm results: avg AUROC: 0.6104033555224962\n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [02:50<00:00,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FO Norm results: avg AUROC: 0.5362631833298946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cam_results = []\n",
    "cam_results_ap = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "#     if (i==1):\n",
    "#         continue\n",
    "    \n",
    "    # model = Network()\n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "\n",
    "    # model.load_state_dict(torch.load(f\"./ckpt/synmol_simpleNet_{i}.pt\"))\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n",
    "\n",
    "    auroc_list = []\n",
    "    auroc_list_alpha = []\n",
    "    auroc_list_beta = []\n",
    "    auroc_list_alpha_beta = []\n",
    "    auroc_list_alpha_beta_ = []\n",
    "    \n",
    "    ap_list = []\n",
    "    \n",
    "    for idx, test_batch in enumerate(tqdm(test_loader)):\n",
    "        if test_batch.y > 1:\n",
    "            continue\n",
    "        \n",
    "#         node_importance_gt = test_batch.xai_labels\n",
    "        \n",
    "        test_batch = test_batch.cuda()\n",
    "        test_batch.pos.requires_grad=True\n",
    "        grad_cam = model.Grad_CAM(test_batch)\n",
    "        node_importance_pred = grad_cam[-2].sum(1).detach().cpu()\n",
    "#         grad_cam = [d.sum(1) for d in grad_cam]\n",
    "#         node_importance_pred = torch.stack(grad_cam).sum(0).detach().cpu()\n",
    "        \n",
    "        \n",
    "        xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "        if (test_batch.y == 0):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "        elif (test_batch.y == 1):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "        \n",
    "        node_importance_gt = xai_labels\n",
    "        \n",
    "        try:\n",
    "            auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())\n",
    "            ap_score = average_precision_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())\n",
    "            \n",
    "            auroc_list.append(auroc_score)\n",
    "            ap_list.append(ap_score)\n",
    "        except:\n",
    "            print(idx)\n",
    "        if test_batch.y == 0:\n",
    "            auroc_list_alpha.append(auroc_score)\n",
    "        elif test_batch.y == 1:\n",
    "            auroc_list_beta.append(auroc_score)\n",
    "        elif test_batch.y == 2:\n",
    "            auroc_list_alpha_beta.append(auroc_score)\n",
    "        elif test_batch.y == 3:\n",
    "            auroc_list_alpha_beta_.append(auroc_score)\n",
    "            \n",
    "    \n",
    "    print(f\"FO Norm results: avg AUROC: {np.mean(auroc_list)}\")\n",
    "    cam_results.append( (np.mean(auroc_list) ))\n",
    "    cam_results_ap.append( (np.mean(ap_list) ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bbcbc73-830a-420d-a96a-dcd488ae5d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5957609518808956, 0.04384209623982976)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cam_results), np.std(cam_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58a80ba5-9cfc-472c-8c10-86e3218df633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6126228257286245, 0.019937647062169044)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cam_results_ap), np.std(cam_results_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d562e99-af47-496b-b83d-4a131bb13a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70006b2-f01d-46f8-9509-e5d3d800f0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29249a3e-d020-4d57-b222-d33aa181fdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7093584583136993"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l = 3 \n",
    "# avg AUROC: 0.7408974059654257\n",
    "np.mean(cam_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd63f185-6ea6-4261-8002-64d7722844b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7410006031836862"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l = 2\n",
    "np.mean(cam_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58598d5c-5a67-4c2c-b61c-3ddf27d79b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "187ba988-9d83-4120-adbd-035d13ba7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LRI_Bern(torch.nn.Module):\n",
    "    def __init__(self, l = 3) -> None:\n",
    "        super().__init__()\n",
    "        self.num_neighbors = 13.67 # typical number of neighbors\n",
    "        self.irreps_sh = o3.Irreps.spherical_harmonics(l)\n",
    "\n",
    "        irreps = self.irreps_sh \n",
    "\n",
    "        if (l == 3):\n",
    "            hidden_irreps = o3.Irreps(\"8x0o+8x0e+8x1o+8x1e+8x2o+8x2e+8x3o+8x3e\")\n",
    "        elif (l==2):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\")\n",
    "        elif (l==4):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e+16x3o+16x3e+16x4o+16x4e\")\n",
    "        elif (l==8):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e+16x3o+16x3e+16x4o+16x4e+16x5o+16x5e+16x6o+16x6e+16x7o+16x7e+16x8o+16x8e\")\n",
    "        elif (l==11):\n",
    "            hidden_irreps = o3.Irreps(\"8x0o+8x0e+8x1o+8x1e+8x2o+8x2e+8x3o+8x3e+8x4o+8x4e+8x5o+8x5e+8x6o+8x6e+8x7o+8x7e+8x8o+8x8e+8x9o+8x9e+8x10o+8x10e+8x11o+8x11e\")#+8x13o+8x13e+8x14o+8x14e+8x15o+8x15e+8x16o+8x16e\")\n",
    "\n",
    "\n",
    "        self.hidden_irreps = hidden_irreps\n",
    "\n",
    "        irreps_input = o3.Irreps(\"17x0e\") + self.irreps_sh \n",
    "        irreps = irreps_input\n",
    "        \n",
    "        normAct = NormActivation(hidden_irreps, torch.sigmoid)\n",
    "        \n",
    "        self.conv1 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.act = normAct\n",
    "        irreps = self.act.irreps_out\n",
    "        self.lin1 = Linear(irreps, irreps)\n",
    "        self.conv2 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.lin2 = Linear(irreps, irreps)\n",
    "        self.conv3 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.lin3 = Linear(irreps, irreps)\n",
    "        # Final layer\n",
    "        self.final = Convolution(irreps, self.irreps_sh, \"1x0e\", self.num_neighbors)\n",
    "        self.irreps_out = self.final.irreps_out\n",
    "\n",
    "    def forward(self, data, mask_order = []) -> torch.Tensor:\n",
    "        num_nodes = 13.67  # typical number of nodes\n",
    "\n",
    "        edge_src, edge_dst = data.edge_index\n",
    "        edge_vec = data.pos[edge_src] - data.pos[edge_dst]\n",
    "        edge_vec = edge_vec.float()\n",
    "        edge_attr = o3.spherical_harmonics(l=self.irreps_sh, x=edge_vec, normalize=True, normalization=\"component\")\n",
    "\n",
    "        for i in mask_order:\n",
    "            edge_attr[:, self.irreps_sh.slices()[i] ] = 0.0\n",
    "\n",
    "        edge_length_embedded = (\n",
    "            soft_one_hot_linspace(x=edge_vec.norm(dim=1), start=0.0, end=max_radius, number=5, basis=\"smooth_finite\", cutoff=True)\n",
    "            * 5**0.5\n",
    "        )\n",
    "\n",
    "        if \"edge_attn\" in data:\n",
    "            edge_attn = data.edge_attn\n",
    "            edge_attr = edge_attr * edge_attn\n",
    "        else:\n",
    "            edge_attn = None\n",
    "\n",
    "        x = scatter(edge_attr, edge_dst, dim=0).div(self.num_neighbors**0.5)\n",
    "\n",
    "        # x = torch.cat( [F.one_hot(data.x, num_classes = 16).squeeze(), x], dim = 1 )\n",
    "        x = torch.cat([data.x, x], dim = 1)\n",
    "\n",
    "        x = self.conv1(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "        \n",
    "        x = self.conv2(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "            \n",
    "        x = self.conv3(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "            \n",
    "        x = self.final(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0e5be97-65bd-4e56-b472-b96e9375a899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [01:52<00:00,  6.40it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FO Norm results: avg AUROC: 0.5205320159647778\n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [01:55<00:00,  6.20it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FO Norm results: avg AUROC: 0.5735052231471075\n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [01:58<00:00,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FO Norm results: avg AUROC: 0.5888301582970572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "lri_b_results = []\n",
    "lri_b_results_ap = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "#     if (i>0):\n",
    "#         continue\n",
    "    \n",
    "    # model = Network()\n",
    "    interpreter = LRI_Bern(l = 3)\n",
    "    interpreter.cuda()\n",
    "\n",
    "    interpreter.load_state_dict(torch.load(f\"./ckpt/foldBackbone_LRI_interpreter_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n",
    "\n",
    "    auroc_list = []\n",
    "    auroc_list_alpha = []\n",
    "    auroc_list_beta = []\n",
    "    auroc_list_alpha_beta = []\n",
    "    auroc_list_alpha_beta_ = []\n",
    "    \n",
    "    ap_list = []\n",
    "    \n",
    "    for idx, test_batch in enumerate(tqdm(test_loader)):\n",
    "        if test_batch.y > 1:\n",
    "            continue\n",
    "        \n",
    "#         node_importance_gt = test_batch.xai_labels\n",
    "        \n",
    "        test_batch = test_batch.cuda()\n",
    "        \n",
    "        output = interpreter(test_batch)\n",
    "    \n",
    "        node_importance_pred = output.sigmoid().detach().cpu()\n",
    "                \n",
    "        xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "        if (test_batch.y == 0):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "        elif (test_batch.y == 1):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "        \n",
    "        node_importance_gt = xai_labels\n",
    "        \n",
    "        try:\n",
    "            auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())\n",
    "            ap_score = average_precision_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())\n",
    "            \n",
    "            auroc_list.append(auroc_score)\n",
    "            ap_list.append(ap_score)\n",
    "        except:\n",
    "            print(idx)\n",
    "            continue\n",
    "        if test_batch.y == 0:\n",
    "            auroc_list_alpha.append(auroc_score)\n",
    "        elif test_batch.y == 1:\n",
    "            auroc_list_beta.append(auroc_score)\n",
    "        elif test_batch.y == 2:\n",
    "            auroc_list_alpha_beta.append(auroc_score)\n",
    "        elif test_batch.y == 3:\n",
    "            auroc_list_alpha_beta_.append(auroc_score)\n",
    "            \n",
    "    \n",
    "    print(f\"FO Norm results: avg AUROC: {np.mean(auroc_list)}\")\n",
    "    lri_b_results.append( (np.mean(auroc_list) ))\n",
    "    lri_b_results_ap.append( (np.mean(ap_list) ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8d2e2d1-23ba-458b-bbe4-02f99d1e9a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5609557991363142, 0.02926061167920771)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lri_b_results), np.std(lri_b_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b681ddab-0e61-48a1-a690-762c01cfe149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5845700366510628, 0.031227877194376044)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lri_b_results_ap), np.std(lri_b_results_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70880711-43fd-4817-a4bb-e9f36a489f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e30dd52-cee2-4d30-92b7-b3c2d0c1f4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25954654713719016"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auroc_list_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810c4b4c-9b72-4e19-9b33-ea46c317300a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6733386690512628"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auroc_list_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70473114-67c3-4761-9b93-60182d3a268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TFN_LRI_Gaussian(torch.nn.Module):\n",
    "    def __init__(self, l = 3) -> None:\n",
    "        super().__init__()\n",
    "        self.num_neighbors = 13.67 # typical number of neighbors\n",
    "        self.irreps_sh = o3.Irreps.spherical_harmonics(l)\n",
    "\n",
    "        irreps = self.irreps_sh \n",
    "\n",
    "        if (l == 3):\n",
    "            hidden_irreps = o3.Irreps(\"8x0o+8x0e+8x1o+8x1e+8x2o+8x2e+8x3o+8x3e\")\n",
    "        elif (l==2):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e\")\n",
    "        elif (l==4):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e+16x3o+16x3e+16x4o+16x4e\")\n",
    "        elif (l==8):\n",
    "            hidden_irreps = o3.Irreps(\"16x0o+16x0e+16x1o+16x1e+16x2o+16x2e+16x3o+16x3e+16x4o+16x4e+16x5o+16x5e+16x6o+16x6e+16x7o+16x7e+16x8o+16x8e\")\n",
    "        elif (l==11):\n",
    "            hidden_irreps = o3.Irreps(\"8x0o+8x0e+8x1o+8x1e+8x2o+8x2e+8x3o+8x3e+8x4o+8x4e+8x5o+8x5e+8x6o+8x6e+8x7o+8x7e+8x8o+8x8e+8x9o+8x9e+8x10o+8x10e+8x11o+8x11e\")#+8x13o+8x13e+8x14o+8x14e+8x15o+8x15e+8x16o+8x16e\")\n",
    "\n",
    "\n",
    "        self.hidden_irreps = hidden_irreps\n",
    "\n",
    "        irreps_input = o3.Irreps(\"17x0e\") + self.irreps_sh \n",
    "        irreps = irreps_input\n",
    "        \n",
    "        normAct = NormActivation(hidden_irreps, torch.sigmoid)\n",
    "        \n",
    "        self.conv1 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.act = normAct\n",
    "        irreps = self.act.irreps_out\n",
    "        self.lin1 = Linear(irreps, irreps)\n",
    "        self.conv2 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.lin2 = Linear(irreps, irreps)\n",
    "        self.conv3 = Convolution(irreps, self.irreps_sh, normAct.irreps_in, self.num_neighbors)\n",
    "        self.lin3 = Linear(irreps, irreps)\n",
    "        # Final layer\n",
    "        self.final = Convolution(irreps, self.irreps_sh, \"11x0e\", self.num_neighbors)\n",
    "        self.irreps_out = self.final.irreps_out\n",
    "\n",
    "    def forward(self, data, mask_order = []) -> torch.Tensor:\n",
    "        num_nodes = 13.67  # typical number of nodes\n",
    "\n",
    "        edge_src, edge_dst = data.edge_index\n",
    "        edge_vec = data.pos[edge_src] - data.pos[edge_dst]\n",
    "        edge_vec = edge_vec.float()\n",
    "        edge_attr = o3.spherical_harmonics(l=self.irreps_sh, x=edge_vec, normalize=True, normalization=\"component\")\n",
    "\n",
    "        for i in mask_order:\n",
    "            edge_attr[:, self.irreps_sh.slices()[i] ] = 0.0\n",
    "\n",
    "        edge_length_embedded = (\n",
    "            soft_one_hot_linspace(x=edge_vec.norm(dim=1), start=0.0, end=max_radius, number=5, basis=\"smooth_finite\", cutoff=True)\n",
    "            * 5**0.5\n",
    "        )\n",
    "\n",
    "        if \"edge_attn\" in data:\n",
    "            edge_attn = data.edge_attn\n",
    "            edge_attr = edge_attr * edge_attn\n",
    "        else:\n",
    "            edge_attn = None\n",
    "\n",
    "        x = scatter(edge_attr, edge_dst, dim=0).div(self.num_neighbors**0.5)\n",
    "\n",
    "        # x = torch.cat( [F.one_hot(data.x, num_classes = 16).squeeze(), x], dim = 1 )\n",
    "        x = torch.cat([data.x, x], dim = 1)\n",
    "\n",
    "        x = self.conv1(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "        \n",
    "        x = self.conv2(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "            \n",
    "        x = self.conv3(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "        x = self.lin3(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        for i in mask_order:\n",
    "            x[:, self.hidden_irreps.slices()[i] ] = 0.0\n",
    "            \n",
    "        x = self.final(x, edge_src, edge_dst, edge_attr, edge_length_embedded, edge_attn = edge_attn)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b9e5768-dd4d-47f1-869b-458a0dcda680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [01:45<00:00,  6.82it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRI results: avg AUROC: 0.6316699491121867\n",
      "LRI results: ap: 0.6093565243970683 \n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [01:32<00:00,  7.75it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRI results: avg AUROC: 0.6171764725719378\n",
      "LRI results: ap: 0.601348541149189 \n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [01:37<00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRI results: avg AUROC: 0.7309368645243514\n",
      "LRI results: ap: 0.7198771858467085 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "\n",
    "lri_g_results = []\n",
    "lri_g_results_ap = []\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    interpreter = TFN_LRI_Gaussian(l = 3)\n",
    "    interpreter.cuda()\n",
    "\n",
    "    interpreter.load_state_dict(torch.load(f\"./ckpt/foldBackbone_LRI_g_interpreter_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    auroc_list = []\n",
    "    auroc_list_alpha = []\n",
    "    auroc_list_beta = []\n",
    "    auroc_list_alpha_beta = []\n",
    "    auroc_list_alpha_beta_ = []\n",
    "    \n",
    "    ap_list = []\n",
    "    \n",
    "    for idx, test_batch in enumerate(tqdm(test_loader)):\n",
    "        if test_batch.y > 1:\n",
    "            continue\n",
    "        \n",
    "        test_batch = test_batch.cuda()\n",
    "        noise = interpreter(test_batch)\n",
    "            \n",
    "        a1 = F.softplus(noise[:,[0]]).clamp(1e-6, 1e6)\n",
    "        a2 = F.softplus(noise[:,[1]]).clamp(1e-6, 1e6) \n",
    "        U = noise[:,2:].reshape(-1,3,3)\n",
    "\n",
    "        pred_sigma = a1.reshape(-1, 1, 1) * U @ U.transpose(1, 2) + a2.reshape(-1, 1, 1) * torch.eye(3, device=noise.device).reshape(-1, 3, 3)\n",
    "        node_importance_pred = -(pred_sigma.det()).reshape(-1).detach().cpu()\n",
    "        \n",
    "        xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "        if (test_batch.y == 0):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "        elif (test_batch.y == 1):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "        \n",
    "        node_importance_gt = xai_labels\n",
    "        \n",
    "        auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.tolist())\n",
    "        ap_score = average_precision_score(node_importance_gt.tolist(), node_importance_pred.tolist())\n",
    "        \n",
    "        auroc_list.append(auroc_score)\n",
    "        ap_list.append(ap_score)\n",
    "        \n",
    "        if test_batch.y == 0:\n",
    "            auroc_list_alpha.append(auroc_score)\n",
    "        elif test_batch.y == 1:\n",
    "            auroc_list_beta.append(auroc_score)\n",
    "        elif test_batch.y == 2:\n",
    "            auroc_list_alpha_beta.append(auroc_score)\n",
    "        elif test_batch.y == 3:\n",
    "            auroc_list_alpha_beta_.append(auroc_score)\n",
    "            \n",
    "            \n",
    "    \n",
    "    print(f\"LRI results: avg AUROC: {np.mean(auroc_list)}\")\n",
    "    print(f\"LRI results: ap: {np.mean(ap_list)} \")\n",
    "\n",
    "    #     LRI_results.append( (np.mean(auroc_list), np.mean(auroc_list_sphere), np.mean(auroc_list_cube)) )\n",
    "    lri_g_results.append( np.mean(auroc_list) )\n",
    "    lri_g_results_ap.append( np.mean(ap_list) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f50f310-fc96-47d4-8623-d79d988f8541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.659927762069492, 0.05055844597518948)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lri_g_results), np.std(lri_g_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccc59c9b-c011-4204-99d1-b6752b55f7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6435274171309886, 0.05408633429454872)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lri_g_results_ap), np.std(lri_g_results_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902d5a1-992b-40f0-a3c7-d530e8c72005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96ca01-19f8-40e9-840a-514b1aad0af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████▍                                   | 313/718 [1:56:31<4:32:03, 40.30s/it]"
     ]
    }
   ],
   "source": [
    "gnn_explainer_results = []\n",
    "gnn_explainer_results_ap = []\n",
    "\n",
    "EPS = 1e-7\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "#     if (i>0):\n",
    "#         break\n",
    "    \n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "\n",
    "    # model.load_state_dict(torch.load(f\"./ckpt/synmol_simpleNet_{i}.pt\"))\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    ## grad test\n",
    "    # val_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    auroc_list = []\n",
    "    auroc_list_alpha = []\n",
    "    auroc_list_beta = []\n",
    "    \n",
    "    ap_list = []\n",
    "    \n",
    "    for test_batch in tqdm(test_loader):\n",
    "        if test_batch.y > 1:\n",
    "            continue\n",
    "            \n",
    "        test_batch = test_batch.cuda()\n",
    "        \n",
    "        edge_mask = torch.nn.Parameter(torch.randn((test_batch.edge_index.shape[1],1), requires_grad=True, device = \"cuda\") * 0.4)\n",
    "        \n",
    "        optimizer = torch.optim.Adam([edge_mask], lr=1e-2)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        pred = model(test_batch)\n",
    "        loss = criterion(pred, test_batch.y)\n",
    "#         print(\"original\", loss.item())\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            test_batch.edge_attn = edge_mask.sigmoid() \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(test_batch)\n",
    "            loss = criterion(pred, test_batch.y)\n",
    "            \n",
    "#             print(\"predict loss\", loss.item())\n",
    "            \n",
    "            m = edge_mask.sigmoid()\n",
    "            loss = loss + 0.0001 * m.sum()\n",
    "            ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "            loss = loss + 0.01 * ent.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        node_importance_pred = edge_mask.data.sigmoid()\n",
    "        node_importance_pred = scatter(node_importance_pred, test_batch.edge_index[1], dim = 0) + scatter(node_importance_pred, test_batch.edge_index[0], dim = 0)\n",
    "        node_importance_pred = node_importance_pred.reshape(-1).detach().cpu() \n",
    "        \n",
    "        xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "        if (test_batch.y == 0):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "        elif (test_batch.y == 1):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "        \n",
    "        node_importance_gt = xai_labels\n",
    "        \n",
    "        auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.tolist())\n",
    "        ap_score = average_precision_score(node_importance_gt.tolist(), node_importance_pred.tolist())\n",
    "        \n",
    "        auroc_list.append(auroc_score)\n",
    "        ap_list.append(ap_score)\n",
    "        \n",
    "        if test_batch.y == 0:\n",
    "            auroc_list_alpha.append(auroc_score)\n",
    "        else:\n",
    "            auroc_list_beta.append(auroc_score)\n",
    "    \n",
    "    print(f\"GNN explainer results: avg AUROC: {np.mean(auroc_list)} , avg AUROC for dodecahedron: {np.mean(auroc_list_alpha)} , avg AUROC for octahedron: {np.mean(auroc_list_beta)}\")\n",
    "    gnn_explainer_results.append( np.mean(auroc_list) )\n",
    "    gnn_explainer_results_ap.append( np.mean(ap_list) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b125e13-c7d6-4fc5-80a3-bc8df7c2a758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7725755639905522, 0.001999958556344492)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gnn_explainer_results), np.std(gnn_explainer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33ad45a1-7040-4940-ad12-136cdca6fbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7237323108504397, 0.00264796165979777)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gnn_explainer_results_ap), np.std(gnn_explainer_results_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db9a74-0b8c-41ef-ae6e-f520e0707463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "718it [20:58,  1.75s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 718/718 [28:00<00:00,  2.34s/it]\n",
      "100%|████████████████████████████████████████████████████████████████| 718/718 [30:42<00:00,  2.57s/it]\n",
      " 39%|█████████████████████████                                       | 281/718 [24:49<39:55,  5.48s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "pg_explainer_results = []\n",
    "pg_explainer_results_ap = []\n",
    "\n",
    "EPS = 1e-7\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "t0 = 5.0\n",
    "t1 = 1.0\n",
    "coff_size = 0.0005\n",
    "coff_ent = 5e-4\n",
    "sample_bias = 0.0\n",
    "\n",
    "def concrete_sample(log_alpha, beta: float = 1.0, training: bool = True):\n",
    "    r\"\"\" Sample from the instantiation of concrete distribution when training \"\"\"\n",
    "    if training:\n",
    "        bias = sample_bias\n",
    "        random_noise = torch.rand(log_alpha.shape) * (1 - 2 * bias) + bias\n",
    "        random_noise = torch.log(random_noise) - torch.log(1.0 - random_noise)\n",
    "        gate_inputs = (random_noise.to(log_alpha.device) + log_alpha) / beta\n",
    "        gate_inputs = gate_inputs.sigmoid()\n",
    "    else:\n",
    "        gate_inputs = log_alpha.sigmoid()\n",
    "\n",
    "    return gate_inputs\n",
    "\n",
    "def pg_loss(prob, ori_pred: int, sparse_mask_values):\n",
    "        logit = prob[ori_pred]\n",
    "        logit = logit + EPS\n",
    "        pred_loss = - torch.log(logit)\n",
    "\n",
    "        # size\n",
    "        edge_mask = sparse_mask_values\n",
    "        size_loss = coff_size * torch.sum(edge_mask)\n",
    "\n",
    "        # entropy\n",
    "        edge_mask = edge_mask * 0.99 + 0.005\n",
    "        mask_ent = - edge_mask * torch.log(edge_mask) - (1 - edge_mask) * torch.log(1 - edge_mask)\n",
    "        mask_ent_loss = coff_ent * torch.mean(mask_ent)\n",
    "\n",
    "        loss = pred_loss + size_loss + mask_ent_loss\n",
    "        return loss\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "\n",
    "    # model.load_state_dict(torch.load(f\"./ckpt/synmol_simpleNet_{i}.pt\"))\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    # val_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    auroc_list = []\n",
    "    auroc_list_cube = []\n",
    "    auroc_list_sphere = []\n",
    "    ap_list = []\n",
    "    ap_list_cube = []\n",
    "    ap_list_sphere = []\n",
    "    \n",
    "    #Train explainer\n",
    "    pg_elayers = FullyConnectedTensorProduct(\n",
    "            irreps_in1=model.act.irreps_out + model.act.irreps_out,\n",
    "            irreps_in2=model.irreps_sh,\n",
    "            irreps_out=\"1x0e\",\n",
    "        )\n",
    "#     Convolution(model.act.irreps_out + model.act.irreps_out, model.irreps_sh, \"1x0e\", model.num_neighbors)\n",
    "    pg_elayers.cuda()\n",
    "    optimizer = torch.optim.Adam(pg_elayers.parameters(), lr=1e-2)\n",
    "    \n",
    "    model.eval()\n",
    "    emb_dict = {}\n",
    "    ori_pred_dict = {}\n",
    "    \n",
    "    for gid, test_batch in tqdm(enumerate(test_loader)):\n",
    "        if test_batch.y > 1:\n",
    "            continue\n",
    "        test_batch = test_batch.cuda()\n",
    "        emb, logits = model.get_emb_logits(test_batch)\n",
    "        emb_dict[gid] = emb.data.cpu()\n",
    "        ori_pred_dict[gid] = logits.argmax(-1).data.cpu()\n",
    "    \n",
    "    print(\"training\")\n",
    "    for epoch in (range(epochs)):\n",
    "        pred_list = []\n",
    "        avg_loss = 0\n",
    "        tmp = float(t0 * np.power(t1 / t0, epoch / epochs))\n",
    "        pg_elayers.train()\n",
    "        optimizer.zero_grad()\n",
    "        for gid, test_batch in enumerate(tqdm(test_loader)):\n",
    "            if test_batch.y > 1:\n",
    "                continue\n",
    "            test_batch = test_batch.cuda()\n",
    "            embed=emb_dict[gid]\n",
    "            col, row = test_batch.edge_index\n",
    "            f1 = embed[col.cpu()]\n",
    "            f2 = embed[row.cpu()]\n",
    "            f12self = torch.cat([f1, f2], dim=-1).cuda()\n",
    "            \n",
    "            data = test_batch\n",
    "            edge_src, edge_dst = data.edge_index\n",
    "            edge_vec = data.pos[edge_src] - data.pos[edge_dst]\n",
    "            edge_attr = o3.spherical_harmonics(l=model.irreps_sh, x=edge_vec, normalize=True, normalization=\"component\")\n",
    "            edge_length_embedded = (\n",
    "                soft_one_hot_linspace(x=edge_vec.norm(dim=1), start=0.0, end=max_radius, number=5, basis=\"smooth_finite\", cutoff=True)\n",
    "                * 5**0.5\n",
    "            )\n",
    "#             print(f12self.device, edge_src.device, edge_dst.device, edge_attr.device, edge_length_embedded.device)\n",
    "            h = pg_elayers(f12self, edge_attr)\n",
    "            values = h.reshape(-1)\n",
    "            values = concrete_sample(values, beta=tmp, training=True)\n",
    "            \n",
    "            nodesize = embed.shape[0]\n",
    "            sparse_mask_values = values\n",
    "            mask_sparse = torch.sparse_coo_tensor(\n",
    "                test_batch.edge_index, values, (nodesize, nodesize)\n",
    "            )\n",
    "            mask_sigmoid = mask_sparse.to_dense()\n",
    "            # set the symmetric edge weights\n",
    "            sym_mask = (mask_sigmoid + mask_sigmoid.transpose(0, 1)) / 2\n",
    "            edge_mask = sym_mask[test_batch.edge_index[0], test_batch.edge_index[1]]\n",
    "            edge_mask = edge_mask.reshape(-1,1)\n",
    "            \n",
    "            test_batch.edge_attn = edge_mask\n",
    "            logits = model(test_batch)\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            loss = pg_loss(probs.squeeze(), ori_pred_dict[gid], sparse_mask_values)\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item()\n",
    "        optimizer.step()\n",
    "#         print(\"epoch \", epoch, \" loss \", avg_loss/len(test_loader))\n",
    "        \n",
    "    for test_batch in tqdm(test_loader):\n",
    "        if test_batch.y > 1:\n",
    "            continue\n",
    "        test_batch = test_batch.cuda()\n",
    "        embed, logits = model.get_emb_logits(test_batch)\n",
    "        col, row = test_batch.edge_index\n",
    "        f1 = embed[col]\n",
    "        f2 = embed[row]\n",
    "        f12self = torch.cat([f1, f2], dim=-1).cuda()\n",
    "        data = test_batch\n",
    "        edge_src, edge_dst = data.edge_index\n",
    "        edge_vec = data.pos[edge_src] - data.pos[edge_dst]\n",
    "        edge_attr = o3.spherical_harmonics(l=model.irreps_sh, x=edge_vec, normalize=True, normalization=\"component\")\n",
    "        edge_length_embedded = (\n",
    "            soft_one_hot_linspace(x=edge_vec.norm(dim=1), start=0.0, end=max_radius, number=5, basis=\"smooth_finite\", cutoff=True)\n",
    "            * 5**0.5\n",
    "        )\n",
    "        \n",
    "        h = pg_elayers(f12self, edge_attr)\n",
    "        values = h.reshape(-1)\n",
    "        values = concrete_sample(values, beta=1.0, training=False)\n",
    "        nodesize = embed.shape[0]\n",
    "        sparse_mask_values = values\n",
    "        mask_sparse = torch.sparse_coo_tensor(\n",
    "            test_batch.edge_index, values, (nodesize, nodesize)\n",
    "        )\n",
    "        mask_sigmoid = mask_sparse.to_dense()\n",
    "        # set the symmetric edge weights\n",
    "        sym_mask = (mask_sigmoid + mask_sigmoid.transpose(0, 1)) / 2\n",
    "        edge_mask = sym_mask[test_batch.edge_index[0], test_batch.edge_index[1]]\n",
    "        edge_mask = edge_mask.reshape(-1,1)\n",
    "            \n",
    "        node_importance_pred = edge_mask\n",
    "        node_importance_pred = scatter(node_importance_pred, test_batch.edge_index[1], dim = 0) + scatter(node_importance_pred, test_batch.edge_index[0], dim = 0)\n",
    "        node_importance_pred = node_importance_pred.reshape(-1).detach().cpu() \n",
    "                \n",
    "        xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "        if (test_batch.y == 0):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "        elif (test_batch.y == 1):\n",
    "            xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "        \n",
    "        node_importance_gt = xai_labels\n",
    "        \n",
    "        try:\n",
    "            auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.tolist())\n",
    "            ap_score = average_precision_score(node_importance_gt.tolist(), node_importance_pred.tolist())\n",
    "        except:\n",
    "            print(test_batch)\n",
    "            continue\n",
    "        \n",
    "        auroc_list.append(auroc_score)\n",
    "        ap_list.append(ap_score)\n",
    "\n",
    "        if test_batch.y == 0:\n",
    "            auroc_list_sphere.append(auroc_score)\n",
    "            ap_list_sphere.append(ap_score)\n",
    "        else:\n",
    "            auroc_list_cube.append(auroc_score)\n",
    "            ap_list_cube.append(ap_score)\n",
    "    \n",
    "    print(f\"PG explainer results: avg AUROC: {np.mean(auroc_list)} , avg AUROC for dodecahedron: {np.mean(auroc_list_sphere)} , avg AUROC for octahedron: {np.mean(auroc_list_cube)}\")\n",
    "    print(f\"Pg explainer results: ap: {np.mean(ap_list)} , ap for dodecahedron: {np.mean(ap_list_sphere)} , ap for octahedron: {np.mean(ap_list_cube)}\")\n",
    "    \n",
    "    pg_explainer_results.append( (np.mean(auroc_list), np.mean(auroc_list_sphere), np.mean(auroc_list_cube)) )\n",
    "    pg_explainer_results_ap.append( (np.mean(ap_list), np.mean(ap_list_sphere), np.mean(ap_list_cube)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6216d4e4-6bae-48a0-9aaa-403567550916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.768666608587852, 0.8745049167513523, 0.6750832624222305),\n",
       " (0.763295711811678, 0.8528157013458604, 0.6841411947498746),\n",
       " (0.7651234630865145, 0.8465173750729769, 0.6931541093300636)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_explainer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e92b28e-ed5a-43ab-92f6-29a5a78cc1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7656952611620148, 0.0022296259513557293)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([i[0] for i in pg_explainer_results]), np.std([i[0] for i in pg_explainer_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81926c1d-8139-4283-9777-663463a1e9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7192103508098682, 0.0012928769238467204)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([i[0] for i in pg_explainer_results_ap]), np.std([i[0] for i in pg_explainer_results_ap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d47748b3-5039-490f-b6fc-d229eeef9a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7263308152959915"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pg_explainer_results_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12bf8961-ef8c-4ccd-8d74-a7820413afc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7630544003232244, 0.8740016257108614, 0.6649536957699452)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_explainer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b63810f2-87b5-4a55-8845-c8549fe8369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 45/45 [00:40<00:00,  1.12it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test ACC 0.8398328690807799, \n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 45/45 [00:41<00:00,  1.09it/s]\n",
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test ACC 0.8454038997214485, \n",
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 45/45 [00:46<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test ACC 0.8454038997214485, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# all_results = []\n",
    "# nodeOnly_results = []\n",
    "# node_edgeAttr_results = []\n",
    "# node_edgeDist_results = []\n",
    "\n",
    "acc = []\n",
    "auroc = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "#     if (i>0):\n",
    "#         continue\n",
    "    \n",
    "    # model = Network()\n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)#, num_workers=16)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(test_loader)):\n",
    "        batch = batch.cuda()\n",
    "        # y =  F.one_hot(batch.y, num_classes = 2)\n",
    "        pred = model(batch).detach()\n",
    "        preds.append( F.softmax(pred, dim = 1).cpu().numpy() )\n",
    "        # preds.append( pred.squeeze().sigmoid().cpu().numpy() )\n",
    "        labels.append( batch.y.reshape(-1).long().detach().cpu().numpy() )\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    print(f\" test ACC { accuracy_score(np.argmax(preds, axis = 1), labels ) }, \")\n",
    "    acc.append(accuracy_score(np.argmax(preds, axis = 1), labels ))\n",
    "#     auroc.append(roc_auc_score(np.argmax(preds, axis = 1), labels ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e3fc8b-247e-463d-b7c2-503d75c97eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8435468895078923, 0.0026262090294765124)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc), np.std(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a7f32a5-f61b-41f3-a655-5b1f5dc728c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 718/718 [03:46<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test ACC 0.4220055710306407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "all_results = []\n",
    "nodeOnly_results = []\n",
    "node_edgeAttr_results = []\n",
    "node_edgeDist_results = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    if (i>0):\n",
    "        continue\n",
    "    \n",
    "    # model = Network()\n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(test_loader)):\n",
    "        batch = batch.cuda()\n",
    "        # y =  F.one_hot(batch.y, num_classes = 2)\n",
    "        \n",
    "        xai_labels = np.zeros(batch.xai_labels_raw[0].shape)\n",
    "        if (batch.y == 0):\n",
    "            xai_labels[batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "        elif (batch.y == 1):\n",
    "            xai_labels[batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "            \n",
    "        pred = model(batch, mask_node = (xai_labels == 1.0).squeeze()).detach()\n",
    "        preds.append( F.softmax(pred, dim = 1).cpu().numpy() )\n",
    "        # preds.append( pred.squeeze().sigmoid().cpu().numpy() )\n",
    "        labels.append( batch.y.reshape(-1).long().detach().cpu().numpy() )\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    print(f\" test ACC { accuracy_score(np.argmax(preds, axis = 1), labels ) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5471ae1-b16d-4930-9e16-eb6e163abe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 718/718 [22:19<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test ACC 0.3203342618384401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "all_results = []\n",
    "nodeOnly_results = []\n",
    "node_edgeAttr_results = []\n",
    "node_edgeDist_results = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    if (i>0):\n",
    "        continue\n",
    "    \n",
    "    # model = Network()\n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(test_loader)):\n",
    "        test_batch = batch.cuda()\n",
    "        # y =  F.one_hot(batch.y, num_classes = 2)\n",
    "        \n",
    "        Rs, Rs_edgeAttr, Rs_edgeDist = model.FirstOrderX( test_batch, R = F.one_hot(test_batch.y, num_classes = 7), zero_point = False, Norm = True)\n",
    "        \n",
    "        R_edgeAttr = torch.sum(torch.stack(Rs_edgeAttr), dim=0) \n",
    "#         R_edgeAttr += Rs[-1]\n",
    "        \n",
    "        R_edgeDist = torch.sum(torch.stack(Rs_edgeDist), dim=0) \n",
    "        \n",
    "        \n",
    "        node_importance_pred = (scatter(R_edgeAttr.sum(dim = 1) + R_edgeDist, test_batch.edge_index[0], dim = 0) + scatter(R_edgeAttr.sum(dim = 1) + R_edgeDist, test_batch.edge_index[1], dim = 0) ) / 2\n",
    "        node_importance_pred += Rs[-1].sum(dim = 1)\n",
    "        \n",
    "        threshold_percentage = 0.2\n",
    "        k = int(len(node_importance_pred) * (threshold_percentage))\n",
    "        top_k_values, _ = torch.topk(node_importance_pred, k)\n",
    "        threshold_value = top_k_values[-1]\n",
    "\n",
    "        pred = model(batch, mask_node = node_importance_pred > threshold_value).detach()\n",
    "        \n",
    "#         xai_labels = np.zeros(batch.xai_labels_raw[0].shape)\n",
    "#         if (batch.y == 0):\n",
    "#             xai_labels[batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "#         elif (batch.y == 1):\n",
    "#             xai_labels[batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "            \n",
    "#         pred = model(batch, mask_node = (xai_labels == 1.0).squeeze()).detach()\n",
    "\n",
    "        preds.append( F.softmax(pred, dim = 1).cpu().numpy() )\n",
    "        labels.append( batch.y.reshape(-1).long().detach().cpu().numpy() )\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    print(f\" test ACC { accuracy_score(np.argmax(preds, axis = 1), labels ) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a913f35a-3335-4d49-91ed-bfdcab0a7d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 718/718 [09:21<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test ACC 0.6225626740947076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "all_results = []\n",
    "nodeOnly_results = []\n",
    "node_edgeAttr_results = []\n",
    "node_edgeDist_results = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    if (i>0):\n",
    "        continue\n",
    "    \n",
    "    # model = Network()\n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(test_loader)):\n",
    "        test_batch = batch.cuda()\n",
    "        # y =  F.one_hot(batch.y, num_classes = 2)\n",
    "\n",
    "        test_batch.pos.requires_grad=True\n",
    "        output = model(test_batch)\n",
    "        model.zero_grad()\n",
    "        output[0, test_batch.y].backward()\n",
    "#         output.sum().backward()\n",
    "        node_importance_pred = test_batch.pos.grad.norm(dim = -1).detach()\n",
    "     \n",
    "        threshold_percentage = 0.2\n",
    "        k = int(len(node_importance_pred) * (threshold_percentage))\n",
    "        top_k_values, _ = torch.topk(node_importance_pred, k)\n",
    "        threshold_value = top_k_values[-1]\n",
    "\n",
    "        pred = model(batch, mask_node = node_importance_pred > threshold_value).detach()\n",
    "        \n",
    "#         xai_labels = np.zeros(batch.xai_labels_raw[0].shape)\n",
    "#         if (batch.y == 0):\n",
    "#             xai_labels[batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "#         elif (batch.y == 1):\n",
    "#             xai_labels[batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "            \n",
    "#         pred = model(batch, mask_node = (xai_labels == 1.0).squeeze()).detach()\n",
    "\n",
    "        preds.append( F.softmax(pred, dim = 1).cpu().numpy() )\n",
    "        labels.append( batch.y.reshape(-1).long().detach().cpu().numpy() )\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    print(f\" test ACC { accuracy_score(np.argmax(preds, axis = 1), labels ) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6909a5c-a667-4a7c-94ac-9acb3bf8cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 718/718 [10:01<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test ACC 0.10027855153203342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "all_results = []\n",
    "nodeOnly_results = []\n",
    "node_edgeAttr_results = []\n",
    "node_edgeDist_results = []\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    if (i>0):\n",
    "        continue\n",
    "    \n",
    "    # model = Network()\n",
    "    model = TFN(l=3)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{i}.pt\"))\n",
    "    \n",
    "    print(\"testing\")\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, batch in enumerate(tqdm(test_loader)):\n",
    "        test_batch = batch.cuda()\n",
    "        # y =  F.one_hot(batch.y, num_classes = 2)\n",
    "        \n",
    "        test_batch = test_batch.cuda()\n",
    "        test_batch.pos.requires_grad=True\n",
    "        grad_cam = model.Grad_CAM(test_batch)\n",
    "        grad_cam = [d.sum(1) for d in grad_cam]\n",
    "        node_importance_pred = torch.stack(grad_cam).sum(0).detach().cpu()\n",
    "     \n",
    "        threshold_percentage = 0.2\n",
    "        k = int(len(node_importance_pred) * (threshold_percentage))\n",
    "        top_k_values, _ = torch.topk(node_importance_pred, k)\n",
    "        threshold_value = top_k_values[-1]\n",
    "\n",
    "        pred = model(batch, mask_node = node_importance_pred > threshold_value).detach()\n",
    "        \n",
    "#         xai_labels = np.zeros(batch.xai_labels_raw[0].shape)\n",
    "#         if (batch.y == 0):\n",
    "#             xai_labels[batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "#         elif (batch.y == 1):\n",
    "#             xai_labels[batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "            \n",
    "#         pred = model(batch, mask_node = (xai_labels == 1.0).squeeze()).detach()\n",
    "\n",
    "        preds.append( F.softmax(pred, dim = 1).cpu().numpy() )\n",
    "        labels.append( batch.y.reshape(-1).long().detach().cpu().numpy() )\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    print(f\" test ACC { accuracy_score(np.argmax(preds, axis = 1), labels ) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763e3fd-d2e7-4274-b9f8-12ba2b908571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78341eed-7750-496c-8366-2caecb02f251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314142e2-f62d-4fed-bff1-b944f497e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_number_to_symbol = {\n",
    "    0: 'H', 1: 'He', 2: 'Li', 3: 'Be', 4: 'B', 5: 'C', 6: 'N', 7: 'O', 8: 'F', 9: 'Ne',\n",
    "    10: 'Na', 11: 'Mg', 12: 'Al', 13: 'Si', 14: 'P', 15: 'S', 16: 'Cl', 17: 'Ar', 18: 'K', 19: 'Ca',\n",
    "    20: 'Sc', 21: 'Ti', 22: 'V', 23: 'Cr', 24: 'Mn', 25: 'Fe', 26: 'Co', 27: 'Ni', 28: 'Cu', 29: 'Zn',\n",
    "    30: 'Ga', 31: 'Ge', 32: 'As', 33: 'Se', 34: 'Br', 35: 'Kr', 36: 'Rb', 37: 'Sr', 38: 'Y', 39: 'Zr',\n",
    "    40: 'Nb', 41: 'Mo', 42: 'Tc', 43: 'Ru', 44: 'Rh', 45: 'Pd', 46: 'Ag', 47: 'Cd', 48: 'In', 49: 'Sn',\n",
    "    50: 'Sb', 51: 'Te', 52: 'I', 53: 'Xe', 54: 'Cs', 55: 'Ba', 56: 'La', 57: 'Ce', 58: 'Pr', 59: 'Nd',\n",
    "    60: 'Pm', 61: 'Sm', 62: 'Eu', 63: 'Gd', 64: 'Tb', 65: 'Dy', 66: 'Ho', 67: 'Er', 68: 'Tm', 69: 'Yb',\n",
    "    70: 'Lu', 71: 'Hf', 72: 'Ta', 73: 'W', 74: 'Re', 75: 'Os', 76: 'Ir', 77: 'Pt', 78: 'Au', 79: 'Hg',\n",
    "    80: 'Tl', 81: 'Pb', 82: 'Bi', 83: 'Po', 84: 'At', 85: 'Rn', 86: 'Fr', 87: 'Ra', 88: 'Ac', 89: 'Th',\n",
    "    90: 'Pa', 91: 'U', 92: 'Np', 93: 'Pu', 94: 'Am', 95: 'Cm', 96: 'Bk', 97: 'Cf', 98: 'Es', 99: 'Fm',\n",
    "    100: 'Md', 101: 'No', 102: 'Lr', 103: 'Rf', 104: 'Db', 105: 'Sg', 106: 'Bh', 107: 'Hs', 108: 'Mt',\n",
    "    109: 'Ds', 110: 'Rg', 111: 'Cn', 112: 'Nh', 113: 'Fl', 114: 'Mc', 115: 'Lv', 116: 'Ts', 117: 'Og'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "dc95e351-2dd3-4302-9f97-777f5509995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from Bio.PDB import Structure, Model, Chain, Residue, Atom\n",
    "from Bio.PDB.Polypeptide import three_to_one\n",
    "import numpy as np\n",
    "\n",
    "def create_structure_from_hdf5(hdf5_file_path):\n",
    "    structure = Structure.Structure(\"1\")  # Create a Structure object\n",
    "    model = Model.Model(0)  # Create a Model object\n",
    "    structure.add(model)\n",
    "\n",
    "    with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "        # Assuming the HDF5 file has datasets for coordinates, atom names, residue names, chain IDs, and residue numbers\n",
    "        # The actual implementation may need adjustments based on the HDF5 file structure\n",
    "        coordinates = hdf5_file['atom_pos'][0,:]\n",
    "        atom_names = hdf5_file['atom_names'][:]\n",
    "        residue_names = hdf5_file['atom_residue_names'][:]\n",
    "        chain_ids = hdf5_file['atom_chain_ids'][:]\n",
    "        residue_nums = hdf5_file['atom_residue_id'][:]\n",
    "        atom_types = hdf5_file['atom_types'][:]\n",
    "        \n",
    "        current_chain_id = None\n",
    "        current_chain = None\n",
    "        current_residue = None\n",
    "        \n",
    "        for i, (coord, atom_name, res_name, chain_id, res_num, atom_type) in enumerate(zip(coordinates, atom_names, residue_names, chain_ids, residue_nums, atom_types)):\n",
    "            atom_name = atom_name.decode('utf-8').strip()\n",
    "#             if atom_name not in ['N', 'CA', 'C']:\n",
    "#                 continue\n",
    "            res_name = res_name.decode('utf-8').strip()\n",
    "            chain_id_str = str(chain_id)  # Convert integer chain_id to string\n",
    "            \n",
    "            # Check if we need to start a new chain\n",
    "            if current_chain_id != chain_id:\n",
    "                current_chain_id = chain_id\n",
    "                if current_chain is not None:\n",
    "                    model.add(current_chain)\n",
    "                current_chain = Chain.Chain(chain_id_str)\n",
    "                current_residue = None  # Reset current residue\n",
    "            \n",
    "            # Check if we need to start a new residue\n",
    "            if current_residue is None or current_residue.get_id()[1] != res_num:\n",
    "                current_residue = Residue.Residue((' ', res_num, ' '), res_name, ' ')\n",
    "                current_chain.add(current_residue)\n",
    "            \n",
    "            # Create and add the atom to the current residue\n",
    "            atom = Atom.Atom(name = atom_name, coord = coord, bfactor = 1.0, occupancy = 1.0, altloc = ' ', fullname = atom_name,element = atomic_number_to_symbol[atom_type], serial_number = i+1)\n",
    "            \n",
    "#             if atom_name in ['N', 'CA', 'C']:\n",
    "#                 new_bfactor = np.random.rand()\n",
    "#                 atom.set_bfactor(new_bfactor)\n",
    "#             else:\n",
    "#                 atom.set_bfactor(0.0)\n",
    "                \n",
    "            new_bfactor = np.random.rand()\n",
    "            atom.set_bfactor(new_bfactor)\n",
    "            \n",
    "            current_residue.add(atom)\n",
    "        \n",
    "        # Don't forget to add the last chain to the model\n",
    "        if current_chain is not None:\n",
    "            model.add(current_chain)\n",
    "\n",
    "    return structure\n",
    "\n",
    "# Example usage\n",
    "# hdf5_file_path = '/data/hongyiling/XAI4e3nn/dataset/scope/training/d2gdma_.hdf5'\n",
    "# structure = create_structure_from_hdf5(hdf5_file_path)\n",
    "\n",
    "# Now, `structure` is a Biopython Structure object that you can manipulate or analyze.\n",
    "# For example, you could compute distances between atoms, analyze structural motifs, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5bcabed8-9e04-4241-9a34-5fefcccbb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_path = '/data/hongyiling/XAI4e3nn/dataset/scope/test_fold/d1m3ya2.hdf5'\n",
    "structure = create_structure_from_hdf5(hdf5_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a6d2ecb3-c7e3-4426-bfe1-f0761c35dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBIO\n",
    "\n",
    "io = PDBIO()\n",
    "io.set_structure(structure)\n",
    "io.save(\"testBeta5.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014b7666-f0f0-44df-a90e-2d4b3ca546bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Structure id=1>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4e88ae0-c243-4259-8c10-26c391166462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.scope.dataloader import FOLDdataset, FOLDBackBonedataset\n",
    "\n",
    "from torch_geometric.transforms import RadiusGraph, KNNGraph\n",
    "\n",
    "# max_radius = 5\n",
    "# Trans = RadiusGraph(r = max_radius)\n",
    "# dataset = FOLDBackBonedataset(root='/data/hongyiling/XAI4e3nn/dataset/scope/', split=\"training\", transform = Trans)\n",
    "test_dataset = FOLDBackBonedataset(root='/data/hongyiling/XAI4e3nn/dataset/scope/', split=\"test_fold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30db172f-1f0d-4ec3-b780-af546ffbf80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[414, 17], pos=[414, 3], xai_labels_raw=[414, 1], xai_labels=[414, 1], id='d1kf6b1', y=[1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95e2ddd8-a177-4724-bc6d-3731f02be2d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['alpha'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None'],\n",
       "       ['None']], dtype='<U5')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0].xai_labels_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "633a5e29-3a2c-4e5a-9451-f1bb41d0f3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d1kf6b1'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "046e65a1-2b2c-4847-866f-02823852bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dssp_result = dssp_dict_from_pdb_file(\"test2.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761fe12-cab5-45ae-932e-641658b7e9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99777f8a-5db6-4126-b301-c8af74b8f920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc6a29f0-ec34-41c5-8395-fd8b2477c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBIO, DSSP\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "# from tempfile import NamedTemporaryFile\n",
    "from Bio.PDB.DSSP import dssp_dict_from_pdb_file\n",
    "import os\n",
    "\n",
    "# Function to run DSSP and check for alpha helices and beta sheets\n",
    "def assign_secondary_structure_with_dssp(pdb_file_path, dssp_executable_path, residue_num):\n",
    "    parser = PDBParser()\n",
    "    structure = parser.get_structure('temp_structure', pdb_file_path)\n",
    "    model = structure[0]  # Assuming you want to analyze the first model\n",
    "    \n",
    "    dssp_result = dssp_dict_from_pdb_file(pdb_file_path)\n",
    "    \n",
    "    return dssp_result\n",
    "\n",
    "    # Variables to track presence of alpha helices and beta sheets\n",
    "    \n",
    "    # Analyze DSSP results\n",
    "    label = [\"None\"] * residue_num\n",
    "    for res_key, res_info in dssp_result[0].items():\n",
    "        if res_info[1] in ['H', 'G', 'I']:  # Alpha helices\n",
    "            label[res_key[1][1]] = 'alpha'\n",
    "#             label.append(\"alpha\")\n",
    "        elif res_info[1] in ['E', 'B']:  # Beta strands (sheets)\n",
    "            label[res_key[1][1]] = 'beta'\n",
    "#             label.append(\"beta\")      \n",
    "#         else:\n",
    "#             label.append(\"None\")\n",
    "        \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e3aa6cd5-8a0a-4309-83d5-1b247ee10a79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: DSSP could not be created due to an error:\n",
      "empty protein, or no valid complete residues\n",
      "\n",
      "  warnings.warn(err)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "DSSP failed to produce an output",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dssp_executable_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdssp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m output_pdb_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.pdb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m dssp_result \u001b[38;5;241m=\u001b[39m \u001b[43massign_secondary_structure_with_dssp\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_pdb_file_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdssp_executable_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m, in \u001b[0;36massign_secondary_structure_with_dssp\u001b[0;34m(pdb_file_path, dssp_executable_path, residue_num)\u001b[0m\n\u001b[1;32m     10\u001b[0m structure \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mget_structure(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_structure\u001b[39m\u001b[38;5;124m'\u001b[39m, pdb_file_path)\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m structure[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming you want to analyze the first model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m dssp_result \u001b[38;5;241m=\u001b[39m \u001b[43mdssp_dict_from_pdb_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdb_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dssp_result\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Variables to track presence of alpha helices and beta sheets\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Analyze DSSP results\u001b[39;00m\n",
      "File \u001b[0;32m/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/Bio/PDB/DSSP.py:201\u001b[0m, in \u001b[0;36mdssp_dict_from_pdb_file\u001b[0;34m(in_file, DSSP, dssp_version)\u001b[0m\n\u001b[1;32m    199\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(err)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m--> 201\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDSSP failed to produce an output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m out_dict, keys \u001b[38;5;241m=\u001b[39m _make_dssp_dict(StringIO(out))\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_dict, keys\n",
      "\u001b[0;31mException\u001b[0m: DSSP failed to produce an output"
     ]
    }
   ],
   "source": [
    "dssp_executable_path = 'mkdssp'\n",
    "output_pdb_file_name = 'test.pdb'\n",
    "dssp_result = assign_secondary_structure_with_dssp(output_pdb_file_name, dssp_executable_path, residue_num = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "606670f5-9e67-4005-a8cc-ffbea91ebc7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dssp_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a39a434-d7c8-4f12-8ab1-94c395755ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_infos = []\n",
    "for res_key, res_info in dssp_result[0].items():\n",
    "    res_infos.append(res_info[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1520c8e-451b-4366-b35a-2553fb68487c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8011b68b-dc5f-44b5-a132-185ded9a993a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', (' ', 152, ' '))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c2cf7f31-3b5b-4276-931f-42454ad1c522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-', '-', '-', '-', 'H', 'H', 'H', 'H', 'H', 'H']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_infos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a296a90-387d-45c6-aeac-c59805a13e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = structure[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "017be968-a329-497a-b50f-9ee662a568b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Model.get_chains at 0x7efee2d59b30>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "63af37e2-5bef-45e9-bda5-894e705fe254",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Residue GLY het=  resseq=0 icode= >\n",
      "<Residue ALA het=  resseq=1 icode= >\n",
      "<Residue LEU het=  resseq=2 icode= >\n",
      "<Residue THR het=  resseq=3 icode= >\n",
      "<Residue GLU het=  resseq=4 icode= >\n",
      "<Residue SER het=  resseq=5 icode= >\n",
      "<Residue GLN het=  resseq=6 icode= >\n",
      "<Residue ALA het=  resseq=7 icode= >\n",
      "<Residue ALA het=  resseq=8 icode= >\n",
      "<Residue LEU het=  resseq=9 icode= >\n",
      "<Residue VAL het=  resseq=10 icode= >\n",
      "<Residue LYS het=  resseq=11 icode= >\n",
      "<Residue SER het=  resseq=12 icode= >\n",
      "<Residue SER het=  resseq=13 icode= >\n",
      "<Residue TRP het=  resseq=14 icode= >\n",
      "<Residue GLU het=  resseq=15 icode= >\n",
      "<Residue GLU het=  resseq=16 icode= >\n",
      "<Residue PHE het=  resseq=17 icode= >\n",
      "<Residue ASN het=  resseq=18 icode= >\n",
      "<Residue ALA het=  resseq=19 icode= >\n",
      "<Residue ASN het=  resseq=20 icode= >\n",
      "<Residue ILE het=  resseq=21 icode= >\n",
      "<Residue PRO het=  resseq=22 icode= >\n",
      "<Residue LYS het=  resseq=23 icode= >\n",
      "<Residue HIS het=  resseq=24 icode= >\n",
      "<Residue THR het=  resseq=25 icode= >\n",
      "<Residue HIS het=  resseq=26 icode= >\n",
      "<Residue ARG het=  resseq=27 icode= >\n",
      "<Residue PHE het=  resseq=28 icode= >\n",
      "<Residue PHE het=  resseq=29 icode= >\n",
      "<Residue ILE het=  resseq=30 icode= >\n",
      "<Residue LEU het=  resseq=31 icode= >\n",
      "<Residue VAL het=  resseq=32 icode= >\n",
      "<Residue LEU het=  resseq=33 icode= >\n",
      "<Residue GLU het=  resseq=34 icode= >\n",
      "<Residue ILE het=  resseq=35 icode= >\n",
      "<Residue ALA het=  resseq=36 icode= >\n",
      "<Residue PRO het=  resseq=37 icode= >\n",
      "<Residue ALA het=  resseq=38 icode= >\n",
      "<Residue ALA het=  resseq=39 icode= >\n",
      "<Residue LYS het=  resseq=40 icode= >\n",
      "<Residue ASP het=  resseq=41 icode= >\n",
      "<Residue LEU het=  resseq=42 icode= >\n",
      "<Residue PHE het=  resseq=43 icode= >\n",
      "<Residue SER het=  resseq=44 icode= >\n",
      "<Residue PHE het=  resseq=45 icode= >\n",
      "<Residue LEU het=  resseq=46 icode= >\n",
      "<Residue LYS het=  resseq=47 icode= >\n",
      "<Residue GLY het=  resseq=48 icode= >\n",
      "<Residue THR het=  resseq=49 icode= >\n",
      "<Residue SER het=  resseq=50 icode= >\n",
      "<Residue GLU het=  resseq=51 icode= >\n",
      "<Residue VAL het=  resseq=52 icode= >\n",
      "<Residue PRO het=  resseq=53 icode= >\n",
      "<Residue GLN het=  resseq=54 icode= >\n",
      "<Residue ASN het=  resseq=55 icode= >\n",
      "<Residue ASN het=  resseq=56 icode= >\n",
      "<Residue PRO het=  resseq=57 icode= >\n",
      "<Residue GLU het=  resseq=58 icode= >\n",
      "<Residue LEU het=  resseq=59 icode= >\n",
      "<Residue GLN het=  resseq=60 icode= >\n",
      "<Residue ALA het=  resseq=61 icode= >\n",
      "<Residue HIS het=  resseq=62 icode= >\n",
      "<Residue ALA het=  resseq=63 icode= >\n",
      "<Residue GLY het=  resseq=64 icode= >\n",
      "<Residue LYS het=  resseq=65 icode= >\n",
      "<Residue VAL het=  resseq=66 icode= >\n",
      "<Residue PHE het=  resseq=67 icode= >\n",
      "<Residue LYS het=  resseq=68 icode= >\n",
      "<Residue LEU het=  resseq=69 icode= >\n",
      "<Residue VAL het=  resseq=70 icode= >\n",
      "<Residue TYR het=  resseq=71 icode= >\n",
      "<Residue GLU het=  resseq=72 icode= >\n",
      "<Residue ALA het=  resseq=73 icode= >\n",
      "<Residue ALA het=  resseq=74 icode= >\n",
      "<Residue ILE het=  resseq=75 icode= >\n",
      "<Residue GLN het=  resseq=76 icode= >\n",
      "<Residue LEU het=  resseq=77 icode= >\n",
      "<Residue GLU het=  resseq=78 icode= >\n",
      "<Residue VAL het=  resseq=79 icode= >\n",
      "<Residue THR het=  resseq=80 icode= >\n",
      "<Residue GLY het=  resseq=81 icode= >\n",
      "<Residue VAL het=  resseq=82 icode= >\n",
      "<Residue VAL het=  resseq=83 icode= >\n",
      "<Residue VAL het=  resseq=84 icode= >\n",
      "<Residue THR het=  resseq=85 icode= >\n",
      "<Residue ASP het=  resseq=86 icode= >\n",
      "<Residue ALA het=  resseq=87 icode= >\n",
      "<Residue THR het=  resseq=88 icode= >\n",
      "<Residue LEU het=  resseq=89 icode= >\n",
      "<Residue LYS het=  resseq=90 icode= >\n",
      "<Residue ASN het=  resseq=91 icode= >\n",
      "<Residue LEU het=  resseq=92 icode= >\n",
      "<Residue GLY het=  resseq=93 icode= >\n",
      "<Residue SER het=  resseq=94 icode= >\n",
      "<Residue VAL het=  resseq=95 icode= >\n",
      "<Residue HIS het=  resseq=96 icode= >\n",
      "<Residue VAL het=  resseq=97 icode= >\n",
      "<Residue SER het=  resseq=98 icode= >\n",
      "<Residue LYS het=  resseq=99 icode= >\n",
      "<Residue GLY het=  resseq=100 icode= >\n",
      "<Residue VAL het=  resseq=101 icode= >\n",
      "<Residue ALA het=  resseq=102 icode= >\n",
      "<Residue ASP het=  resseq=103 icode= >\n",
      "<Residue ALA het=  resseq=104 icode= >\n",
      "<Residue HIS het=  resseq=105 icode= >\n",
      "<Residue PHE het=  resseq=106 icode= >\n",
      "<Residue PRO het=  resseq=107 icode= >\n",
      "<Residue VAL het=  resseq=108 icode= >\n",
      "<Residue VAL het=  resseq=109 icode= >\n",
      "<Residue LYS het=  resseq=110 icode= >\n",
      "<Residue GLU het=  resseq=111 icode= >\n",
      "<Residue ALA het=  resseq=112 icode= >\n",
      "<Residue ILE het=  resseq=113 icode= >\n",
      "<Residue LEU het=  resseq=114 icode= >\n",
      "<Residue LYS het=  resseq=115 icode= >\n",
      "<Residue THR het=  resseq=116 icode= >\n",
      "<Residue ILE het=  resseq=117 icode= >\n",
      "<Residue LYS het=  resseq=118 icode= >\n",
      "<Residue GLU het=  resseq=119 icode= >\n",
      "<Residue VAL het=  resseq=120 icode= >\n",
      "<Residue VAL het=  resseq=121 icode= >\n",
      "<Residue GLY het=  resseq=122 icode= >\n",
      "<Residue ALA het=  resseq=123 icode= >\n",
      "<Residue LYS het=  resseq=124 icode= >\n",
      "<Residue TRP het=  resseq=125 icode= >\n",
      "<Residue SER het=  resseq=126 icode= >\n",
      "<Residue GLU het=  resseq=127 icode= >\n",
      "<Residue GLU het=  resseq=128 icode= >\n",
      "<Residue LEU het=  resseq=129 icode= >\n",
      "<Residue ASN het=  resseq=130 icode= >\n",
      "<Residue SER het=  resseq=131 icode= >\n",
      "<Residue ALA het=  resseq=132 icode= >\n",
      "<Residue TRP het=  resseq=133 icode= >\n",
      "<Residue THR het=  resseq=134 icode= >\n",
      "<Residue ILE het=  resseq=135 icode= >\n",
      "<Residue ALA het=  resseq=136 icode= >\n",
      "<Residue TYR het=  resseq=137 icode= >\n",
      "<Residue ASP het=  resseq=138 icode= >\n",
      "<Residue GLU het=  resseq=139 icode= >\n",
      "<Residue LEU het=  resseq=140 icode= >\n",
      "<Residue ALA het=  resseq=141 icode= >\n",
      "<Residue ILE het=  resseq=142 icode= >\n",
      "<Residue VAL het=  resseq=143 icode= >\n",
      "<Residue ILE het=  resseq=144 icode= >\n",
      "<Residue LYS het=  resseq=145 icode= >\n",
      "<Residue LYS het=  resseq=146 icode= >\n",
      "<Residue GLU het=  resseq=147 icode= >\n",
      "<Residue MET het=  resseq=148 icode= >\n",
      "<Residue ASP het=  resseq=149 icode= >\n",
      "<Residue ASP het=  resseq=150 icode= >\n",
      "<Residue ALA het=  resseq=151 icode= >\n",
      "<Residue ALA het=  resseq=152 icode= >\n"
     ]
    }
   ],
   "source": [
    "for chain in structure[0]:\n",
    "    for residue in chain:\n",
    "        print(residue)\n",
    "#     print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d80086c8-0a55-46b3-a984-82bb20c0d65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4c7dc8cb-d138-433f-a472-7efcf413c1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A', '-', 115, -8.8, 360.0, 153, -2, -0.8, -1, -0.1, -3, -0.5, -3, -0.1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dssp_result[0][(chain.id, residue.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "243c4c58-0720-4a0c-a20b-777aac3e5a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' ', 152, ' ')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residue.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0493b26b-da19-4563-bd27-c2baab761411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Residue GLY het=  resseq=0 icode= >"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce2341ac-2c99-4ff9-a736-a8dde84ac1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(1, 'RES', 'A', 5, 'RES', 'A', 19, 1),\n",
       "  (2, 'RES', 'A', 22, 'RES', 'A', 36, 1),\n",
       "  (3, 'RES', 'A', 38, 'RES', 'A', 43, 5),\n",
       "  (4, 'RES', 'A', 58, 'RES', 'A', 81, 1),\n",
       "  (5, 'RES', 'A', 88, 'RES', 'A', 99, 1),\n",
       "  (6, 'RES', 'A', 104, 'RES', 'A', 106, 5),\n",
       "  (7, 'RES', 'A', 107, 'RES', 'A', 122, 1),\n",
       "  (8, 'RES', 'A', 123, 'RES', 'A', 125, 5),\n",
       "  (9, 'RES', 'A', 128, 'RES', 'A', 151, 1)],\n",
       " [])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_pdb_records(structure_list):\n",
    "    helix_counter = 1\n",
    "    sheet_counter = 1\n",
    "    helix_records = []\n",
    "    sheet_records = []\n",
    "    start = 0\n",
    "    current_type = structure_list[0]\n",
    "\n",
    "    for i in range(1, len(structure_list) + 1):\n",
    "        if i == len(structure_list) or structure_list[i] != current_type:\n",
    "            if current_type in ['H', 'G', 'I']:  # Helix types\n",
    "                helix_class = {'H': 1, 'G': 5, 'I': 3}.get(current_type, 1)\n",
    "                helix_records.append((helix_counter, 'RES', 'A', start + 1, 'RES', 'A', i, helix_class))\n",
    "                helix_counter += 1\n",
    "            elif current_type in ['E', 'B']:  # Sheet types\n",
    "                sheet_records.append((sheet_counter, 'Sheet1', 2, 'RES', 'A', start + 1, 'RES', 'A', i, 1, 0, '', '', '', '', '', ''))\n",
    "                sheet_counter += 1\n",
    "            start = i\n",
    "            if i < len(structure_list):\n",
    "                current_type = structure_list[i]\n",
    "\n",
    "    return helix_records, sheet_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "461a57bd-b736-40ed-8f1e-56a920635807",
   "metadata": {},
   "outputs": [],
   "source": [
    "helix_records, sheet_records = generate_pdb_records(res_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c413d2d-2cb2-4944-9582-a18e110b0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_helix_sheet_records(filename, helix_records, sheet_records):\n",
    "    with open(filename, 'a') as file:\n",
    "        # Write HELIX records\n",
    "        for helix in helix_records:\n",
    "            length_of_helix = helix[6] - helix[3] + 1  # Calculating length of the helix\n",
    "            file.write(\n",
    "                f\"HELIX  {helix[0]:>3} {'H' + str(helix[0]):<3} {helix[1]:<3} {helix[2]} \"\n",
    "                f\"{helix[3]:>4}    {helix[4]:<3} {helix[5]} {helix[6]:>4}    {helix[7]:>2}   \"\n",
    "                f\"{'':<30}{length_of_helix:>5}\\n\"\n",
    "            )\n",
    "\n",
    "        # Write SHEET records, assuming just one strand per sheet as example\n",
    "        for sheet in sheet_records:\n",
    "            file.write(\n",
    "                f\"SHEET  {sheet[0]:>3} {'S' + str(sheet[0]):<3} 1  {sheet[1]:<3} {sheet[2]} \"\n",
    "                f\"{sheet[3]:>4}    {sheet[4]:<3} {sheet[5]} {sheet[6]:>4}    0  \"\n",
    "                f\"{'':>4} {'':>3} {'':1} {'':>4} {'':1} {'':>4} {'':>3} {'':1} {'':>4} {'':1}\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef609e44-4a14-4b07-ac06-6341d422d624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'RES', 'A', 5, 'RES', 'A', 19, 1),\n",
       " (2, 'RES', 'A', 22, 'RES', 'A', 36, 1),\n",
       " (3, 'RES', 'A', 38, 'RES', 'A', 43, 5),\n",
       " (4, 'RES', 'A', 58, 'RES', 'A', 81, 1),\n",
       " (5, 'RES', 'A', 88, 'RES', 'A', 99, 1),\n",
       " (6, 'RES', 'A', 104, 'RES', 'A', 106, 5),\n",
       " (7, 'RES', 'A', 107, 'RES', 'A', 122, 1),\n",
       " (8, 'RES', 'A', 123, 'RES', 'A', 125, 5),\n",
       " (9, 'RES', 'A', 128, 'RES', 'A', 151, 1)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helix_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb04da9e-06d5-41a9-a4d6-ec5ca0490708",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_helix_sheet_records(\"test6.pdb\", helix_records, sheet_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b090d-b061-4bff-b021-94fc7715b7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce6364-c4f2-4190-ba55-e48936c1abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_path = Path(\"/data/hongyiling/XAI4e3nn/dataset/scope/test_fold\")\n",
    "\n",
    "all_files = list(root_path.rglob('*.hdf5'))\n",
    "\n",
    "all_files = [str(file) for file in all_files if file.is_file()]\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    try:\n",
    "        structure = create_structure_from_hdf5(file)\n",
    "        residue_num = len([1 for _ in structure.get_residues()])\n",
    "        \n",
    "        dssp_executable_path = 'mkdssp'\n",
    "        output_pdb_file_name = 'output_structure.pdb'\n",
    "\n",
    "        write_structure_to_pdb(structure, output_pdb_file_name)\n",
    "\n",
    "        # Run DSSP analysis\n",
    "        label = assign_secondary_structure_with_dssp(output_pdb_file_name, dssp_executable_path, residue_num = residue_num)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        assert(len(label) == residue_num)\n",
    "\n",
    "        output_file_name = file[:-5]\n",
    "    #         print(output_file_name)\n",
    "        np.save(output_file_name, label)\n",
    "        \n",
    "    except:\n",
    "        print(file)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4956af-7e18-43c0-9c65-02f5653943b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477b2c4-b432-44f7-84fa-8b1696906c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8dafe514-354b-4d10-b387-218aea2aa8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBIO, DSSP\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "# from tempfile import NamedTemporaryFile\n",
    "from Bio.PDB.DSSP import dssp_dict_from_pdb_file\n",
    "import os\n",
    "\n",
    "hdf5_file_path = '/data/hongyiling/XAI4e3nn/dataset/scope/training/d2gdma_.hdf5'\n",
    "structure = create_structure_from_hdf5(hdf5_file_path)\n",
    "\n",
    "io = PDBIO()\n",
    "io.set_structure(structure)\n",
    "io.save(\"temp.pdb\")\n",
    "\n",
    "backbonestructure = create_structure_from_hdf5(hdf5_file_path, backbone = True)\n",
    "\n",
    "dssp_result = dssp_dict_from_pdb_file(\"temp.pdb\")\n",
    "\n",
    "SecondaryStructureLines = []\n",
    "\n",
    "model = structure[0]\n",
    "helix_cnt = 0\n",
    "sheet_cnt = 0\n",
    "current_type = None\n",
    "end_id, end_resname = None, None\n",
    "\n",
    "for chain in model:\n",
    "    for residue in chain:\n",
    "        if current_type == None:\n",
    "            current_type = dssp_result[0][(chain.id, residue.id)][1]\n",
    "            start_id = residue.id[1]\n",
    "            start_resname = residue.resname\n",
    "        elif current_type == dssp_result[0][(chain.id, residue.id)][1]:\n",
    "            end_id = residue.id[1]\n",
    "            end_resname = residue.resname\n",
    "        elif current_type != dssp_result[0][(chain.id, residue.id)][1]:\n",
    "            if current_type in ['H', 'G', 'I'] and end_id is not None: # Helix types\n",
    "                helix_id = f\"H{helix_cnt + 1}\"\n",
    "                helix_class = {'H': 1, 'G': 5, 'I': 3}.get(current_type, 1)\n",
    "                SecondaryStructureLines.append(f\"HELIX  {helix_cnt+1: >3} {helix_id: >3} {start_resname} {chain.id}  {start_id: >3}  {end_resname} {chain.id}  {end_id: >3} {helix_class: >2}\\n\")\n",
    "                helix_cnt += 1\n",
    "                \n",
    "            current_type = dssp_result[0][(chain.id, residue.id)][1]\n",
    "            start_id = residue.id[1]\n",
    "            start_resname = residue.resname\n",
    "            end_id, end_resname = None, None\n",
    "\n",
    "\n",
    "io = PDBIO()\n",
    "io.set_structure(backbonestructure)\n",
    "io.save(\"temp.pdb\")\n",
    "\n",
    "with open(\"temp.pdb\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "lines[-1:-1] = SecondaryStructureLines\n",
    "\n",
    "# Write the modified lines back to the file\n",
    "with open(\"test7.pdb\", 'w') as file:\n",
    "    file.writelines(lines)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c6b7ab-ab8a-407b-b276-40f521a5444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81db3947-d37e-4729-bbc9-4af55e7275a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_number_to_symbol = {\n",
    "    0: 'H', 1: 'He', 2: 'Li', 3: 'Be', 4: 'B', 5: 'C', 6: 'N', 7: 'O', 8: 'F', 9: 'Ne',\n",
    "    10: 'Na', 11: 'Mg', 12: 'Al', 13: 'Si', 14: 'P', 15: 'S', 16: 'Cl', 17: 'Ar', 18: 'K', 19: 'Ca',\n",
    "    20: 'Sc', 21: 'Ti', 22: 'V', 23: 'Cr', 24: 'Mn', 25: 'Fe', 26: 'Co', 27: 'Ni', 28: 'Cu', 29: 'Zn',\n",
    "    30: 'Ga', 31: 'Ge', 32: 'As', 33: 'Se', 34: 'Br', 35: 'Kr', 36: 'Rb', 37: 'Sr', 38: 'Y', 39: 'Zr',\n",
    "    40: 'Nb', 41: 'Mo', 42: 'Tc', 43: 'Ru', 44: 'Rh', 45: 'Pd', 46: 'Ag', 47: 'Cd', 48: 'In', 49: 'Sn',\n",
    "    50: 'Sb', 51: 'Te', 52: 'I', 53: 'Xe', 54: 'Cs', 55: 'Ba', 56: 'La', 57: 'Ce', 58: 'Pr', 59: 'Nd',\n",
    "    60: 'Pm', 61: 'Sm', 62: 'Eu', 63: 'Gd', 64: 'Tb', 65: 'Dy', 66: 'Ho', 67: 'Er', 68: 'Tm', 69: 'Yb',\n",
    "    70: 'Lu', 71: 'Hf', 72: 'Ta', 73: 'W', 74: 'Re', 75: 'Os', 76: 'Ir', 77: 'Pt', 78: 'Au', 79: 'Hg',\n",
    "    80: 'Tl', 81: 'Pb', 82: 'Bi', 83: 'Po', 84: 'At', 85: 'Rn', 86: 'Fr', 87: 'Ra', 88: 'Ac', 89: 'Th',\n",
    "    90: 'Pa', 91: 'U', 92: 'Np', 93: 'Pu', 94: 'Am', 95: 'Cm', 96: 'Bk', 97: 'Cf', 98: 'Es', 99: 'Fm',\n",
    "    100: 'Md', 101: 'No', 102: 'Lr', 103: 'Rf', 104: 'Db', 105: 'Sg', 106: 'Bh', 107: 'Hs', 108: 'Mt',\n",
    "    109: 'Ds', 110: 'Rg', 111: 'Cn', 112: 'Nh', 113: 'Fl', 114: 'Mc', 115: 'Lv', 116: 'Ts', 117: 'Og'\n",
    "}\n",
    "\n",
    "import h5py\n",
    "from Bio.PDB import Structure, Model, Chain, Residue, Atom\n",
    "from Bio.PDB.Polypeptide import three_to_one\n",
    "import numpy as np\n",
    "\n",
    "def create_structure_from_hdf5(hdf5_file_path, backbone = False, xai_values = None):\n",
    "    structure = Structure.Structure(\"1\")  # Create a Structure object\n",
    "    model = Model.Model(0)  # Create a Model object\n",
    "    structure.add(model)\n",
    "\n",
    "    with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "        # Assuming the HDF5 file has datasets for coordinates, atom names, residue names, chain IDs, and residue numbers\n",
    "        # The actual implementation may need adjustments based on the HDF5 file structure\n",
    "        coordinates = hdf5_file['atom_pos'][0,:]\n",
    "        atom_names = hdf5_file['atom_names'][:]\n",
    "        residue_names = hdf5_file['atom_residue_names'][:]\n",
    "        chain_ids = hdf5_file['atom_chain_ids'][:]\n",
    "        residue_nums = hdf5_file['atom_residue_id'][:]\n",
    "        atom_types = hdf5_file['atom_types'][:]\n",
    "        \n",
    "        current_chain_id = None\n",
    "        current_chain = None\n",
    "        current_residue = None\n",
    "        \n",
    "        idx = 0\n",
    "        \n",
    "        for i, (coord, atom_name, res_name, chain_id, res_num, atom_type) in enumerate(zip(coordinates, atom_names, residue_names, chain_ids, residue_nums, atom_types)):\n",
    "            atom_name = atom_name.decode('utf-8').strip()\n",
    "            if backbone:\n",
    "                if atom_name not in ['N', 'CA', 'C']:\n",
    "                    continue\n",
    "            res_name = res_name.decode('utf-8').strip()\n",
    "            chain_id_str = str(chain_id)  # Convert integer chain_id to string\n",
    "            \n",
    "            # Check if we need to start a new chain\n",
    "            if current_chain_id != chain_id:\n",
    "                current_chain_id = chain_id\n",
    "                if current_chain is not None:\n",
    "                    model.add(current_chain)\n",
    "                current_chain = Chain.Chain(chain_id_str)\n",
    "                current_residue = None  # Reset current residue\n",
    "            \n",
    "            # Check if we need to start a new residue\n",
    "            if current_residue is None or current_residue.get_id()[1] != res_num:\n",
    "                current_residue = Residue.Residue((' ', res_num, ' '), res_name, ' ')\n",
    "                current_chain.add(current_residue)\n",
    "            \n",
    "            # Create and add the atom to the current residue\n",
    "            atom = Atom.Atom(name = atom_name, coord = coord, bfactor = 1.0, occupancy = 1.0, altloc = ' ', fullname = atom_name,element = atomic_number_to_symbol[atom_type], serial_number = i+1)\n",
    "            \n",
    "            if xai_values:\n",
    "                atom.set_bfactor(xai_values[idx])\n",
    "                idx += 1\n",
    "                \n",
    "#             if atom_name in ['N', 'CA', 'C']:\n",
    "#                 new_bfactor = np.random.rand()\n",
    "#                 atom.set_bfactor(new_bfactor)\n",
    "            \n",
    "            current_residue.add(atom)\n",
    "        \n",
    "        # Don't forget to add the last chain to the model\n",
    "        if current_chain is not None:\n",
    "            model.add(current_chain)\n",
    "\n",
    "    return structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "113a8aa4-9bfd-478b-aa9d-47cb287a9c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELIX    1  H1 ALA 0 152'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helix_cnt = 0\n",
    "helix_id = f\"H{helix_cnt + 1}\"\n",
    "f\"HELIX  {helix_cnt+1: >3} {helix_id: >3} {residue.resname} {chain.id} {residue.id[1]:>3}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ac382624-34d6-4b41-9862-4d2b68f92374",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_path = '/data/hongyiling/XAI4e3nn/dataset/scope/test_fold/d1m3ya2.hdf5'\n",
    "structure = create_structure_from_hdf5(hdf5_file_path, backbone = True)\n",
    "\n",
    "io = PDBIO()\n",
    "io.set_structure(structure)\n",
    "io.save(\"testBeta2.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0cd91e19-4542-437d-840d-4ec7ca842f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ab035-3ae4-444e-9141-b1df8854a77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e054663-69be-493c-8efa-1994cfb965d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e33faae-f1c7-4670-a9bb-95566aefa220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBIO, DSSP\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "# from tempfile import NamedTemporaryFile\n",
    "from Bio.PDB.DSSP import dssp_dict_from_pdb_file\n",
    "import os\n",
    "\n",
    "def visualize_xai_results(fileID, Xscores, outfileName):\n",
    "    hdf5_file_path = f'/data/hongyiling/XAI4e3nn/dataset/scope/test_fold/{fileID}.hdf5'\n",
    "    structure = create_structure_from_hdf5(hdf5_file_path)\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(\"temp.pdb\")\n",
    "\n",
    "    backbonestructure = create_structure_from_hdf5(hdf5_file_path, backbone = True, xai_values = Xscores)\n",
    "\n",
    "    dssp_result = dssp_dict_from_pdb_file(\"temp.pdb\")\n",
    "\n",
    "    SecondaryStructureLines = []\n",
    "\n",
    "    model = structure[0]\n",
    "    helix_cnt = 0\n",
    "    sheet_cnt = 0\n",
    "    current_type = None\n",
    "    end_id, end_resname = None, None\n",
    "\n",
    "    for chain in model:\n",
    "        for residue in chain:\n",
    "            if current_type == None:\n",
    "                current_type = dssp_result[0][(chain.id, residue.id)][1]\n",
    "                start_id = residue.id[1]\n",
    "                start_resname = residue.resname\n",
    "            elif current_type == dssp_result[0][(chain.id, residue.id)][1]:\n",
    "                end_id = residue.id[1]\n",
    "                end_resname = residue.resname\n",
    "            elif current_type != dssp_result[0][(chain.id, residue.id)][1]:\n",
    "                if current_type in ['H', 'G', 'I'] and end_id is not None: # Helix types\n",
    "                    helix_id = f\"H{helix_cnt + 1}\"\n",
    "                    helix_class = {'H': 1, 'G': 5, 'I': 3}.get(current_type, 1)\n",
    "                    SecondaryStructureLines.append(f\"HELIX  {helix_cnt+1: >3} {helix_id: >3} {start_resname} {chain.id}  {start_id: >3}  {end_resname} {chain.id}  {end_id: >3} {helix_class: >2}\\n\")\n",
    "                    helix_cnt += 1\n",
    "\n",
    "                current_type = dssp_result[0][(chain.id, residue.id)][1]\n",
    "                start_id = residue.id[1]\n",
    "                start_resname = residue.resname\n",
    "                end_id, end_resname = None, None\n",
    "    \n",
    "    io = PDBIO()\n",
    "    io.set_structure(backbonestructure)\n",
    "    io.save(\"temp.pdb\")\n",
    "\n",
    "    with open(\"temp.pdb\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    lines[-1:-1] = SecondaryStructureLines\n",
    "\n",
    "    # Write the modified lines back to the file\n",
    "    with open(f\"{outfileName}.pdb\", 'w') as file:\n",
    "        file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15606dff-186b-42ed-9d27-ee91c069e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.scope.dataloader import FOLDdataset, FOLDBackBonedataset\n",
    "\n",
    "from torch_geometric.transforms import RadiusGraph, KNNGraph\n",
    "\n",
    "max_radius = 5\n",
    "Trans = RadiusGraph(r = max_radius)\n",
    "test_dataset = FOLDBackBonedataset(root='/data/hongyiling/XAI4e3nn/dataset/scope/', split=\"test_fold\", transform = Trans)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)#, num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b21f6ac-a22a-4c7a-bf21-4681c109ac73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFN(l=3)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./ckpt/foldBackbone_TFN_l=3_{0}.pt\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4770f0-7141-4f7c-9a40-cebd5d31a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "test_batch = Batch.from_data_list([test_dataset[9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37d4429-9152-4028-bed8-2211d8ce4043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[972, 17], pos=[972, 3], xai_labels_raw=[1], xai_labels=[972, 1], id=[1], y=[1], edge_index=[2, 12616], batch=[972], ptr=[2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5a3554-7726-4dbc-88a2-b5623790a8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d1gxma_']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "03273eb3-2f81-4c97-9e86-8e74bb25b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FirstOrder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "Rs, Rs_edgeAttr, Rs_edgeDist = model.FirstOrderX( test_batch, R = F.one_hot(test_batch.y, num_classes = 7), zero_point = False, Norm = True)\n",
    "        \n",
    "R_edgeAttr = torch.sum(torch.stack(Rs_edgeAttr), dim=0) \n",
    "#         R_edgeAttr += Rs[-1]\n",
    "\n",
    "R_edgeDist = torch.sum(torch.stack(Rs_edgeDist), dim=0) \n",
    "\n",
    "xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "if (test_batch.y == 0):\n",
    "    xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "elif (test_batch.y == 1):\n",
    "    xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "node_importance_gt = xai_labels\n",
    "\n",
    "node_importance_pred = (scatter(R_edgeAttr.sum(dim = 1) + R_edgeDist, test_batch.edge_index[0], dim = 0) + scatter(R_edgeAttr.sum(dim = 1) + R_edgeDist, test_batch.edge_index[1], dim = 0) ) / 2\n",
    "node_importance_pred += Rs[-1].sum(dim = 1)\n",
    "\n",
    "auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e78dff83-3ff3-48fc-b22a-3ee16444f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "test_batch.pos.requires_grad=True\n",
    "output = model(test_batch)\n",
    "model.zero_grad()\n",
    "output[0, test_batch.y].backward()\n",
    "#         output.sum().backward()\n",
    "node_importance_pred = test_batch.pos.grad.norm(dim = -1).detach().cpu()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da2e261c-6db2-4767-ac86-47490ace6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAM\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "test_batch.pos.requires_grad=True\n",
    "grad_cam = model.Grad_CAM(test_batch)\n",
    "node_importance_pred = grad_cam[-2].sum(1).detach().cpu()\n",
    "# node_importance_pred = torch.stack(grad_cam).sum(0).detach().cpu()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d79586ab-8db4-4a05-b831-ddfce0c0a5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LRI Bern\n",
    "interpreter = LRI_Bern(l = 3)\n",
    "interpreter.cuda()\n",
    "\n",
    "interpreter.load_state_dict(torch.load(f\"./ckpt/foldBackbone_LRI_interpreter_TFN_l=3_{0}.pt\"))\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "        \n",
    "output = interpreter(test_batch)\n",
    "\n",
    "node_importance_pred = output.sigmoid().detach().cpu().squeeze()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14a428b5-5a6a-44a5-8e90-140e5a915c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    }
   ],
   "source": [
    "# LRI Gaussian\n",
    "interpreter = TFN_LRI_Gaussian(l = 3)\n",
    "interpreter.cuda()\n",
    "\n",
    "interpreter.load_state_dict(torch.load(f\"./ckpt/foldBackbone_LRI_g_interpreter_TFN_l=3_{0}.pt\"))\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "noise = interpreter(test_batch)\n",
    "a1 = F.softplus(noise[:,[0]]).clamp(1e-6, 1e6)\n",
    "a2 = F.softplus(noise[:,[1]]).clamp(1e-6, 1e6) \n",
    "U = noise[:,2:].reshape(-1,3,3)\n",
    "\n",
    "pred_sigma = a1.reshape(-1, 1, 1) * U @ U.transpose(1, 2) + a2.reshape(-1, 1, 1) * torch.eye(3, device=noise.device).reshape(-1, 3, 3)\n",
    "node_importance_pred = -(pred_sigma.det()).reshape(-1).detach().cpu()\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a52e91c-d183-4b43-a204-8b25e04a15a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# GNN Explainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "epochs = 100\n",
    "EPS = 1e-7\n",
    "\n",
    "edge_mask = torch.nn.Parameter(torch.randn((test_batch.edge_index.shape[1],1), requires_grad=True, device = \"cuda\") * 0.4)\n",
    "        \n",
    "optimizer = torch.optim.Adam([edge_mask], lr=1e-2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "pred = model(test_batch)\n",
    "loss = criterion(pred, test_batch.y)\n",
    "#         print(\"original\", loss.item())\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    test_batch.edge_attn = edge_mask.sigmoid() \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred = model(test_batch)\n",
    "    loss = criterion(pred, test_batch.y)\n",
    "\n",
    "#             print(\"predict loss\", loss.item())\n",
    "\n",
    "    m = edge_mask.sigmoid()\n",
    "    loss = loss + 0.0001 * m.sum()\n",
    "    ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "    loss = loss + 0.01 * ent.mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "node_importance_pred = edge_mask.data.sigmoid()\n",
    "node_importance_pred = scatter(node_importance_pred, test_batch.edge_index[1], dim = 0) + scatter(node_importance_pred, test_batch.edge_index[0], dim = 0)\n",
    "node_importance_pred = node_importance_pred.reshape(-1).detach().cpu() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081877ee-d61b-4501-88a1-e3f7274632e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495330b6-8ce1-47a4-963c-c9bb49aa7285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "ccd40a5c-e7ed-46b7-bdd0-b2997ae564ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9027718365847863"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "91df79fd-7623-4b64-bca5-f31c798de5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.895)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2, 0.715\n",
    "3, 0.9288\n",
    "4, 0.8741\n",
    "5, 0.895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb11aa7f-2ead-4108-9b3e-601bf2d1cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_values = node_importance_pred.detach().cpu().tolist()\n",
    "min_value = min(xai_values)\n",
    "max_value = max(xai_values)\n",
    "\n",
    "# Scale the values to the range [0, 1]\n",
    "scaled_xai_values = [ 5 * (x - min_value) / (max_value - min_value) for x in xai_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "c08ed1fa-1a00-4495-a5cb-69cb78fccb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_xai_results(test_batch.id[0], scaled_xai_values, \"./scop_visualization/\" +test_batch.id[0] + \"FirstOrderX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87c96fe6-590a-45d5-9429-4de7d557403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_xai_results(test_batch.id[0], scaled_xai_values, \"./scop_visualization/\" +test_batch.id[0] + \"Grad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2dc7e54f-7135-4bc5-b6bb-1478aa400cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_xai_results(test_batch.id[0], scaled_xai_values, \"./scop_visualization/\" +test_batch.id[0] + \"CAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8b87ced-8efb-43ad-bb7d-3bdc48ace9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_xai_results(test_batch.id[0], scaled_xai_values, \"./scop_visualization/\" +test_batch.id[0] + \"LRI_Bern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c55a4a1-e755-4778-92d6-85a697e5f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_xai_results(test_batch.id[0], scaled_xai_values, \"./scop_visualization/\" +test_batch.id[0] + \"LRI_Gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b569e5a2-b490-4337-a0bf-6d6abbca3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_xai_results(test_batch.id[0], scaled_xai_values, \"./scop_visualization/\" +test_batch.id[0] + \"GNNExplainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b65f2bf-00ac-44a0-8675-bec403e13a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_number_to_symbol = {\n",
    "    0: 'H', 1: 'He', 2: 'Li', 3: 'Be', 4: 'B', 5: 'C', 6: 'N', 7: 'O', 8: 'F', 9: 'Ne',\n",
    "    10: 'Na', 11: 'Mg', 12: 'Al', 13: 'Si', 14: 'P', 15: 'S', 16: 'Cl', 17: 'Ar', 18: 'K', 19: 'Ca',\n",
    "    20: 'Sc', 21: 'Ti', 22: 'V', 23: 'Cr', 24: 'Mn', 25: 'Fe', 26: 'Co', 27: 'Ni', 28: 'Cu', 29: 'Zn',\n",
    "    30: 'Ga', 31: 'Ge', 32: 'As', 33: 'Se', 34: 'Br', 35: 'Kr', 36: 'Rb', 37: 'Sr', 38: 'Y', 39: 'Zr',\n",
    "    40: 'Nb', 41: 'Mo', 42: 'Tc', 43: 'Ru', 44: 'Rh', 45: 'Pd', 46: 'Ag', 47: 'Cd', 48: 'In', 49: 'Sn',\n",
    "    50: 'Sb', 51: 'Te', 52: 'I', 53: 'Xe', 54: 'Cs', 55: 'Ba', 56: 'La', 57: 'Ce', 58: 'Pr', 59: 'Nd',\n",
    "    60: 'Pm', 61: 'Sm', 62: 'Eu', 63: 'Gd', 64: 'Tb', 65: 'Dy', 66: 'Ho', 67: 'Er', 68: 'Tm', 69: 'Yb',\n",
    "    70: 'Lu', 71: 'Hf', 72: 'Ta', 73: 'W', 74: 'Re', 75: 'Os', 76: 'Ir', 77: 'Pt', 78: 'Au', 79: 'Hg',\n",
    "    80: 'Tl', 81: 'Pb', 82: 'Bi', 83: 'Po', 84: 'At', 85: 'Rn', 86: 'Fr', 87: 'Ra', 88: 'Ac', 89: 'Th',\n",
    "    90: 'Pa', 91: 'U', 92: 'Np', 93: 'Pu', 94: 'Am', 95: 'Cm', 96: 'Bk', 97: 'Cf', 98: 'Es', 99: 'Fm',\n",
    "    100: 'Md', 101: 'No', 102: 'Lr', 103: 'Rf', 104: 'Db', 105: 'Sg', 106: 'Bh', 107: 'Hs', 108: 'Mt',\n",
    "    109: 'Ds', 110: 'Rg', 111: 'Cn', 112: 'Nh', 113: 'Fl', 114: 'Mc', 115: 'Lv', 116: 'Ts', 117: 'Og'\n",
    "}\n",
    "\n",
    "import h5py\n",
    "from Bio.PDB import Structure, Model, Chain, Residue, Atom\n",
    "from Bio.PDB.Polypeptide import three_to_one\n",
    "import numpy as np\n",
    "\n",
    "def create_structure_from_hdf5(hdf5_file_path, xai_values = None):\n",
    "    structure = Structure.Structure(\"1\")  # Create a Structure object\n",
    "    model = Model.Model(0)  # Create a Model object\n",
    "    structure.add(model)\n",
    "\n",
    "    with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "        # Assuming the HDF5 file has datasets for coordinates, atom names, residue names, chain IDs, and residue numbers\n",
    "        # The actual implementation may need adjustments based on the HDF5 file structure\n",
    "        coordinates = hdf5_file['atom_pos'][0,:]\n",
    "        atom_names = hdf5_file['atom_names'][:]\n",
    "        residue_names = hdf5_file['atom_residue_names'][:]\n",
    "        chain_ids = hdf5_file['atom_chain_ids'][:]\n",
    "        residue_nums = hdf5_file['atom_residue_id'][:]\n",
    "        atom_types = hdf5_file['atom_types'][:]\n",
    "        \n",
    "        current_chain_id = None\n",
    "        current_chain = None\n",
    "        current_residue = None\n",
    "        \n",
    "        idx = 0\n",
    "        \n",
    "        residue_xai_value = []\n",
    "        \n",
    "        for i, (coord, atom_name, res_name, chain_id, res_num, atom_type) in enumerate(zip(coordinates, atom_names, residue_names, chain_ids, residue_nums, atom_types)):\n",
    "            atom_name = atom_name.decode('utf-8').strip()\n",
    "#             print(atom_name)\n",
    "#             if backbone:\n",
    "            \n",
    "            res_name = res_name.decode('utf-8').strip()\n",
    "            chain_id_str = str(chain_id)  # Convert integer chain_id to string\n",
    "            \n",
    "            # Check if we need to start a new chain\n",
    "            if current_chain_id != chain_id:\n",
    "                current_chain_id = chain_id\n",
    "                if current_chain is not None:\n",
    "                    model.add(current_chain)\n",
    "                current_chain = Chain.Chain(chain_id_str)\n",
    "                current_residue = None  # Reset current residue\n",
    "            \n",
    "            # Check if we need to start a new residue\n",
    "            if current_residue is None or current_residue.get_id()[1] != res_num:\n",
    "                current_residue = Residue.Residue((' ', res_num, ' '), res_name, ' ')\n",
    "                current_chain.add(current_residue)\n",
    "                residue_xai_value = []\n",
    "            \n",
    "            # Create and add the atom to the current residue\n",
    "            atom = Atom.Atom(name = atom_name, coord = coord, bfactor = 1.0, occupancy = 1.0, altloc = ' ', fullname = atom_name,element = atomic_number_to_symbol[atom_type], serial_number = i+1)\n",
    "            \n",
    "            if xai_values:\n",
    "                if atom_name in ['N', 'CA', 'C']:\n",
    "                    residue_xai_value.append(xai_values[idx])\n",
    "                    atom.set_bfactor(xai_values[idx])\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    atom.set_bfactor(np.mean(residue_xai_value))\n",
    "                    \n",
    "                \n",
    "#             if atom_name in ['N', 'CA', 'C']:\n",
    "#                 new_bfactor = np.random.rand()\n",
    "#                 atom.set_bfactor(new_bfactor)\n",
    "            \n",
    "            current_residue.add(atom)\n",
    "        \n",
    "#         print(len(residue_xai_value))\n",
    "        # Don't forget to add the last chain to the model\n",
    "        if current_chain is not None:\n",
    "            model.add(current_chain)\n",
    "\n",
    "    return structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49af08fe-1c1b-425b-9362-4e06bd8a88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "test_batch = Batch.from_data_list([test_dataset[202]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb65dcba-f458-444b-b1a0-dedea88c5d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[420, 17], pos=[420, 3], xai_labels_raw=[1], xai_labels=[420, 1], id=[1], y=[1], edge_index=[2, 4874], batch=[420], ptr=[2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57344007-2446-4638-8f01-4f5435042da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d9d4c75-a156-4c9e-bdd8-a4fd23ebc749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d1hx6a2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee341901-fecf-4b9d-8ebb-e67ed1945619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FirstOrder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "Rs, Rs_edgeAttr, Rs_edgeDist = model.FirstOrderX( test_batch, R = F.one_hot(test_batch.y, num_classes = 7), zero_point = False, Norm = True)\n",
    "        \n",
    "R_edgeAttr = torch.sum(torch.stack(Rs_edgeAttr), dim=0) \n",
    "#         R_edgeAttr += Rs[-1]\n",
    "\n",
    "R_edgeDist = torch.sum(torch.stack(Rs_edgeDist), dim=0) \n",
    "\n",
    "xai_labels = np.zeros(test_batch.xai_labels_raw[0].shape)\n",
    "if (test_batch.y == 0):\n",
    "    xai_labels[test_batch.xai_labels_raw[0] == 'alpha'] = 1.0\n",
    "elif (test_batch.y == 1):\n",
    "    xai_labels[test_batch.xai_labels_raw[0] == 'beta'] = 1.0 \n",
    "node_importance_gt = xai_labels\n",
    "\n",
    "node_importance_pred = (scatter(R_edgeAttr.sum(dim = 1) + R_edgeDist, test_batch.edge_index[0], dim = 0) + scatter(R_edgeAttr.sum(dim = 1) + R_edgeDist, test_batch.edge_index[1], dim = 0) ) / 2\n",
    "node_importance_pred += Rs[-1].sum(dim = 1)\n",
    "\n",
    "auroc_score = roc_auc_score(node_importance_gt.tolist(), node_importance_pred.detach().cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c1576a5-c534-4b6f-9fb6-c586e456042b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8934469410659887"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1119d5bb-90aa-4ad8-83eb-92ae22dafdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_values = node_importance_pred.detach().cpu().tolist()\n",
    "min_value = min(xai_values)\n",
    "max_value = max(xai_values)\n",
    "\n",
    "# Scale the values to the range [0, 1]\n",
    "scaled_xai_values = [ 5 * (x - min_value) / (max_value - min_value) for x in xai_values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dddac06-07d1-4ff3-a24b-93410b0b0fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d1hx6a2'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b47fbd97-8530-4c8c-b90a-cc761f1e27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_path = f'/data/hongyiling/XAI4e3nn/dataset/scope/test_fold/{test_batch.id[0]}.hdf5'\n",
    "structure = create_structure_from_hdf5(hdf5_file_path, scaled_xai_values)\n",
    "io = PDBIO()\n",
    "io.set_structure(structure)\n",
    "outfileName = \"./scop_visualization/\" +test_batch.id[0] + \"FirstOrderX\"\n",
    "io.save(f\"{outfileName}.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58012f5f-5864-4fe3-804c-045766607cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "test_batch.pos.requires_grad=True\n",
    "output = model(test_batch)\n",
    "model.zero_grad()\n",
    "output[0, test_batch.y].backward()\n",
    "#         output.sum().backward()\n",
    "node_importance_pred = test_batch.pos.grad.norm(dim = -1).detach().cpu()\n",
    "\n",
    "xai_values = node_importance_pred.detach().cpu().tolist()\n",
    "min_value = min(xai_values)\n",
    "max_value = max(xai_values)\n",
    "\n",
    "# Scale the values to the range [0, 1]\n",
    "scaled_xai_values = [ 5 * (x - min_value) / (max_value - min_value) for x in xai_values]\n",
    "\n",
    "hdf5_file_path = f'/data/hongyiling/XAI4e3nn/dataset/scope/test_fold/{test_batch.id[0]}.hdf5'\n",
    "structure = create_structure_from_hdf5(hdf5_file_path, scaled_xai_values)\n",
    "io = PDBIO()\n",
    "io.set_structure(structure)\n",
    "outfileName = \"./scop_visualization/\" +test_batch.id[0] + \"Grad\"\n",
    "io.save(f\"{outfileName}.pdb\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "685954e6-3f87-47ab-9f6e-3ab3ff8588a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAM\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "test_batch.pos.requires_grad=True\n",
    "grad_cam = model.Grad_CAM(test_batch)\n",
    "node_importance_pred = grad_cam[-2].sum(1).detach().cpu()\n",
    "\n",
    "xai_values = node_importance_pred.detach().cpu().tolist()\n",
    "min_value = min(xai_values)\n",
    "max_value = max(xai_values)\n",
    "\n",
    "# Scale the values to the range [0, 1]\n",
    "scaled_xai_values = [ 5 * (x - min_value) / (max_value - min_value) for x in xai_values]\n",
    "\n",
    "hdf5_file_path = f'/data/hongyiling/XAI4e3nn/dataset/scope/test_fold/{test_batch.id[0]}.hdf5'\n",
    "structure = create_structure_from_hdf5(hdf5_file_path, scaled_xai_values)\n",
    "io = PDBIO()\n",
    "io.set_structure(structure)\n",
    "outfileName = \"./scop_visualization/\" +test_batch.id[0] + \"CAM\"\n",
    "io.save(f\"{outfileName}.pdb\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a64ea208-4a43-4884-83b9-74428bc45b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/hongyiling/anaconda3/envs/XAI/lib/python3.9/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    }
   ],
   "source": [
    "# LRI Bern\n",
    "interpreter = LRI_Bern(l = 3)\n",
    "interpreter.cuda()\n",
    "\n",
    "interpreter.load_state_dict(torch.load(f\"./ckpt/foldBackbone_LRI_interpreter_TFN_l=3_{0}.pt\"))\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "        \n",
    "output = interpreter(test_batch)\n",
    "\n",
    "node_importance_pred = output.sigmoid().detach().cpu().squeeze()\n",
    "\n",
    "xai_values = node_importance_pred.detach().cpu().tolist()\n",
    "min_value = min(xai_values)\n",
    "max_value = max(xai_values)\n",
    "\n",
    "# Scale the values to the range [0, 1]\n",
    "scaled_xai_values = [ 5 * (x - min_value) / (max_value - min_value) for x in xai_values]\n",
    "\n",
    "hdf5_file_path = f'/data/hongyiling/XAI4e3nn/dataset/scope/test_fold/{test_batch.id[0]}.hdf5'\n",
    "structure = create_structure_from_hdf5(hdf5_file_path, scaled_xai_values)\n",
    "io = PDBIO()\n",
    "io.set_structure(structure)\n",
    "outfileName = \"./scop_visualization/\" +test_batch.id[0] + \"LRI_Bern\"\n",
    "io.save(f\"{outfileName}.pdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1247dbed-599c-4129-a5f0-027aace6ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRI Gaussian\n",
    "interpreter = TFN_LRI_Gaussian(l = 3)\n",
    "interpreter.cuda()\n",
    "\n",
    "interpreter.load_state_dict(torch.load(f\"./ckpt/foldBackbone_LRI_g_interpreter_TFN_l=3_{0}.pt\"))\n",
    "\n",
    "test_batch = test_batch.cuda()\n",
    "noise = interpreter(test_batch)\n",
    "a1 = F.softplus(noise[:,[0]]).clamp(1e-6, 1e6)\n",
    "a2 = F.softplus(noise[:,[1]]).clamp(1e-6, 1e6) \n",
    "U = noise[:,2:].reshape(-1,3,3)\n",
    "\n",
    "pred_sigma = a1.reshape(-1, 1, 1) * U @ U.transpose(1, 2) + a2.reshape(-1, 1, 1) * torch.eye(3, device=noise.device).reshape(-1, 3, 3)\n",
    "node_importance_pred = -(pred_sigma.det()).reshape(-1).detach().cpu()\n",
    "    \n",
    "xai_values = node_importance_pred.detach().cpu().tolist()\n",
    "min_value = min(xai_values)\n",
    "max_value = max(xai_values)\n",
    "\n",
    "# Scale the values to the range [0, 1]\n",
    "scaled_xai_values = [ 5 * (x - min_value) / (max_value - min_value) for x in xai_values]\n",
    "\n",
    "hdf5_file_path = f'/data/hongyiling/XAI4e3nn/dataset/scope/test_fold/{test_batch.id[0]}.hdf5'\n",
    "structure = create_structure_from_hdf5(hdf5_file_path, scaled_xai_values)\n",
    "io = PDBIO()\n",
    "io.set_structure(structure)\n",
    "outfileName = \"./scop_visualization/\" +test_batch.id[0] + \"LRI_Guassian\"\n",
    "io.save(f\"{outfileName}.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "327d1125-29da-4bac-9f23-b3143818ae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 100/100 [00:29<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# GNN Explainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "epochs = 100\n",
    "EPS = 1e-7\n",
    "\n",
    "edge_mask = torch.nn.Parameter(torch.randn((test_batch.edge_index.shape[1],1), requires_grad=True, device = \"cuda\") * 0.4)\n",
    "        \n",
    "optimizer = torch.optim.Adam([edge_mask], lr=1e-2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "pred = model(test_batch)\n",
    "loss = criterion(pred, test_batch.y)\n",
    "#         print(\"original\", loss.item())\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    test_batch.edge_attn = edge_mask.sigmoid() \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred = model(test_batch)\n",
    "    loss = criterion(pred, test_batch.y)\n",
    "\n",
    "#             print(\"predict loss\", loss.item())\n",
    "\n",
    "    m = edge_mask.sigmoid()\n",
    "    loss = loss + 0.0001 * m.sum()\n",
    "    ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "    loss = loss + 0.01 * ent.mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "node_importance_pred = edge_mask.data.sigmoid()\n",
    "node_importance_pred = scatter(node_importance_pred, test_batch.edge_index[1], dim = 0) + scatter(node_importance_pred, test_batch.edge_index[0], dim = 0)\n",
    "node_importance_pred = node_importance_pred.reshape(-1).detach().cpu() \n",
    "\n",
    "xai_values = node_importance_pred.detach().cpu().tolist()\n",
    "min_value = min(xai_values)\n",
    "max_value = max(xai_values)\n",
    "\n",
    "# Scale the values to the range [0, 1]\n",
    "scaled_xai_values = [ 5 * (x - min_value) / (max_value - min_value) for x in xai_values]\n",
    "\n",
    "hdf5_file_path = f'/data/hongyiling/XAI4e3nn/dataset/scope/test_fold/{test_batch.id[0]}.hdf5'\n",
    "structure = create_structure_from_hdf5(hdf5_file_path, scaled_xai_values)\n",
    "io = PDBIO()\n",
    "io.set_structure(structure)\n",
    "outfileName = \"./scop_visualization/\" +test_batch.id[0] + \"GNN_explainer\"\n",
    "io.save(f\"{outfileName}.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b4617-277b-408f-9e62-e25594faea36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
